{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install torch torchvision scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models.google_vit_model import GoogleViTModel\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load precomputed DAM and 3D embeddings\n",
    "BASE_CACHE_DIR = '.cache'\n",
    "selected_model_key = GoogleViTModel().model_name  # adjust if needed\n",
    "BACKGROUND_REMOVAL_METHOD = 'RMBG_2'\n",
    "EMBEDDING_AGGREGRATION_METHOD = \"mean\"\n",
    "\n",
    "# Load DAM embeddings (including 3D augmentations if available)\n",
    "embeddings_file = os.path.join(BASE_CACHE_DIR, \"embeddings\", f'dam_features-{selected_model_key}-{BACKGROUND_REMOVAL_METHOD}.pkl')\n",
    "embeddings_file_3d = os.path.join(BASE_CACHE_DIR, \"embeddings\", f'dam_features-{selected_model_key}-rembg-3d.pkl')\n",
    "\n",
    "with open(embeddings_file, 'rb') as f:\n",
    "    dam_features = pickle.load(f)\n",
    "\n",
    "if os.path.exists(embeddings_file_3d):\n",
    "    with open(embeddings_file_3d, 'rb') as f:\n",
    "        dam_features_3d = pickle.load(f)\n",
    "    # Merge original and 3D features\n",
    "    dam_features.update(dam_features_3d)\n",
    "\n",
    "# Function to aggregate embeddings if needed\n",
    "def aggregate_embedding(embedding):\n",
    "    if len(embedding.shape) >= 2:\n",
    "        # Assuming last dimension tokens, take mean over tokens\n",
    "        embedding = embedding.squeeze()\n",
    "        # embedding = np.mean(embedding, axis=0) # Mean pooling\n",
    "        # embedding = embedding.flatten() # Flatten\n",
    "        \n",
    "        # Reshape into 2D matrix\n",
    "        embedding = embedding[1:]  # Remove CLS token\n",
    "        embedding = embedding.reshape(-1, 14, 14)\n",
    "    else:\n",
    "        embedding = embedding.flatten()\n",
    "    return embedding\n",
    "\n",
    "# Process all embeddings: aggregate them for simplicity\n",
    "for key in list(dam_features.keys()):\n",
    "    dam_features[key] = aggregate_embedding(dam_features[key])\n",
    "    # print(dam_features[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Network Definition\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class EmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten spatial dimensions if needed\n",
    "        return nn.functional.normalize(self.net(x), p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Network Definition\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class EmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dim=512):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # First Convolutional Block\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Second Convolutional Block\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Third Convolutional Block\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling to handle variable input sizes\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fully connected layers for embedding\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = nn.functional.normalize(x, p=2, dim=1)  # L2 normalization\n",
    "        return x\n",
    "\n",
    "model = EmbeddingNetwork(input_channels=1024).to(device)\n",
    "\n",
    "x = list(dam_features.values())[0]\n",
    "x = torch.tensor(x).unsqueeze(0).to(device)\n",
    "\n",
    "print(model(x).shape)\n",
    "\n",
    "# Example verification of identity initialization\n",
    "x = torch.randn(1, 1024, 14, 14).to(device)  # Example input tensor\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "\n",
    "# Since the network reduces the channel dimension from 1024 to 256,\n",
    "# exact identity isn't possible, but you can check if the output retains\n",
    "# the main features of the input.\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Triplet Loss and Dataset Preparation\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "triplet_loss_fn = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, dam_features, anchor_positive_pairs, negative_samples):\n",
    "        \"\"\"\n",
    "        dam_features: dict mapping image paths to embeddings.\n",
    "        anchor_positive_pairs: list of tuples (anchor_path, positive_path).\n",
    "        negative_samples: dict mapping anchor_path -> [one_negative_path].\n",
    "        \"\"\"\n",
    "        self.dam_features = dam_features\n",
    "        self.anchor_positive_pairs = anchor_positive_pairs\n",
    "        self.negative_samples = negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anchor_positive_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_path, positive_path = self.anchor_positive_pairs[idx]\n",
    "        \n",
    "        # Retrieve the single negative path\n",
    "        negatives = self.negative_samples.get(anchor_path, [])\n",
    "        negative_path = negatives[0] if len(negatives) > 0 else positive_path\n",
    "        \n",
    "        anchor_emb = self.dam_features[anchor_path]\n",
    "        positive_emb = self.dam_features[positive_path]\n",
    "        negative_emb = self.dam_features.get(negative_path, positive_emb)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(anchor_emb, dtype=torch.float32),\n",
    "            torch.tensor(positive_emb, dtype=torch.float32),\n",
    "            torch.tensor(negative_emb, dtype=torch.float32)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Create Triplet Data for Training with Mixed Hard Negative Sampling (FAISS)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Parameters\n",
    "NEIGHBOR_SEARCH_K = 10  # How many neighbors to retrieve for each anchor\n",
    "TOP_K = 5                # Number of top neighbors to consider for hard negatives\n",
    "batch_size = 1        # Adjust based on your system's memory capacity\n",
    "\n",
    "# 1) Group 3D augmentation images by their product id\n",
    "product_to_3d = {}\n",
    "for path in tqdm(dam_features.keys(), desc=\"Grouping 3D augmentation images\"):\n",
    "    if \"-\" not in os.path.basename(path):\n",
    "        continue\n",
    "    basename = os.path.basename(path)\n",
    "    prod_code = basename.split(\"-\")[0]\n",
    "    product_to_3d.setdefault(prod_code, []).append(path)\n",
    "\n",
    "# 2) Create anchor-positive pairs (original -> one of its 3D variants)\n",
    "anchor_positive_pairs = []\n",
    "for path in tqdm(list(dam_features.keys()), desc=\"Creating anchor-positive pairs\"):\n",
    "    basename = os.path.basename(path)\n",
    "    if \"-\" in basename:\n",
    "        # Skip 3D variants here\n",
    "        continue\n",
    "    prod_code = basename.split(\".\")[0]\n",
    "    # Filter original images\n",
    "    if \"-\" not in basename or not basename.split(\"-\")[-1].isdigit():\n",
    "        variants = product_to_3d.get(prod_code, [])\n",
    "        for var_path in variants:\n",
    "            if var_path != path:\n",
    "                anchor_positive_pairs.append((path, var_path))\n",
    "\n",
    "print(f\"Total anchor-positive pairs: {len(anchor_positive_pairs)}\")\n",
    "\n",
    "# 3) Build FAISS index for all embeddings in float32\n",
    "all_paths = list(dam_features.keys())\n",
    "\n",
    "# Average embeddings along axis 0 and flatten\n",
    "all_embeddings = np.array([dam_features[p].mean(axis=0).flatten() for p in all_paths], dtype=np.float32)\n",
    "print(f\"All embeddings shape: {all_embeddings.shape}\")  # Should be (N, D)\n",
    "\n",
    "# Create path -> index mapping for O(1) lookups\n",
    "path_to_index = {p: i for i, p in enumerate(all_paths)}\n",
    "\n",
    "# Extract product codes\n",
    "prod_codes = [os.path.basename(p).split(\"-\")[0] for p in all_paths]\n",
    "\n",
    "# Build FAISS index\n",
    "dim = all_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)  # Using Euclidean distance\n",
    "index.add(all_embeddings)\n",
    "print(f\"FAISS index built with {index.ntotal} vectors, dimension={dim}.\")\n",
    "\n",
    "# 4) Precompute possible negatives per product for random sampling\n",
    "# This avoids filtering the entire dataset each time\n",
    "all_negatives = {}\n",
    "for prod in set(prod_codes):\n",
    "    all_negatives[prod] = [p for p, p_code in zip(all_paths, prod_codes) if p_code != prod]\n",
    "\n",
    "# 5) Perform batch FAISS search and mixed negative sampling\n",
    "negative_samples = {}\n",
    "\n",
    "print(\"Performing batch FAISS search for mixed hard negatives...\")\n",
    "for start in tqdm(range(0, len(anchor_positive_pairs), batch_size), desc=\"Batching Anchors\"):\n",
    "    end = min(start + batch_size, len(anchor_positive_pairs))\n",
    "    batch_pairs = anchor_positive_pairs[start:end]\n",
    "    batch_anchor_paths = [pair[0] for pair in batch_pairs]\n",
    "    \n",
    "    # Get anchor indices\n",
    "    batch_anchor_indices = [path_to_index[a_p[0]] for a_p in batch_pairs]\n",
    "    \n",
    "    # Get anchor embeddings for this batch\n",
    "    batch_embs = all_embeddings[batch_anchor_indices]  # Shape: (batch_size, dim)\n",
    "    \n",
    "    # Query FAISS for top NEIGHBOR_SEARCH_K neighbors\n",
    "    # +1 because it can return the anchor itself\n",
    "    D, I = index.search(batch_embs, NEIGHBOR_SEARCH_K + 1)\n",
    "    \n",
    "    for i, (anchor_path, _) in enumerate(batch_pairs):\n",
    "        anchor_idx = batch_anchor_indices[i]\n",
    "        anchor_prod = prod_codes[anchor_idx]\n",
    "        \n",
    "        neighbors = I[i]\n",
    "        # Exclude the anchor itself and same product\n",
    "        valid_neighbors = [idx for idx in neighbors if idx != anchor_idx and prod_codes[idx] != anchor_prod]\n",
    "        \n",
    "        # With 50% probability, choose from top K hard negatives\n",
    "        if len(valid_neighbors) >= TOP_K:\n",
    "            chosen_negative = random.choice(valid_neighbors[:TOP_K])\n",
    "        elif len(valid_neighbors) > 0:\n",
    "            chosen_negative = random.choice(valid_neighbors)\n",
    "        else:\n",
    "            # Fallback: choose a random negative from all_negatives\n",
    "            possible_negatives = all_negatives.get(anchor_prod, [])\n",
    "            if possible_negatives:\n",
    "                chosen_negative = random.choice(possible_negatives)\n",
    "            else:\n",
    "                # If no negatives available, fallback to positive\n",
    "                chosen_negative = anchor_path\n",
    "        \n",
    "        # Decide randomly whether to pick from top K or fully random\n",
    "        if random.random() < 0.5 and len(valid_neighbors) >= TOP_K:\n",
    "            # Pick a hard negative from top K\n",
    "            negative_idx = random.choice(valid_neighbors[:TOP_K])\n",
    "            chosen_negative = all_paths[negative_idx]\n",
    "        else:\n",
    "            # Pick a random negative from all_negatives\n",
    "            possible_negatives = all_negatives.get(anchor_prod, [])\n",
    "            if possible_negatives:\n",
    "                chosen_negative = random.choice(possible_negatives)\n",
    "            else:\n",
    "                # Fallback to positive if no negatives are available\n",
    "                chosen_negative = anchor_path\n",
    "        \n",
    "        # Store the single negative for this anchor\n",
    "        negative_samples[anchor_path] = [chosen_negative]\n",
    "\n",
    "print(\"Mixed hard negative sampling completed!\")\n",
    "print(f\"Total anchor-positive pairs: {len(anchor_positive_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Initialize Network, Loss, and DataLoader for Triplet Training\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "sample_embedding = next(iter(dam_features.values()))\n",
    "input_dim = sample_embedding.shape[0]\n",
    "print(f\"Input dimension: {input_dim}, Sample embedding shape: {sample_embedding.shape}\")\n",
    "\n",
    "embedding_net = EmbeddingNetwork(input_dim).to(device)\n",
    "optimizer = torch.optim.Adam(embedding_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Create dataset and dataloader for triplets\n",
    "triplet_dataset = TripletDataset(dam_features, anchor_positive_pairs, negative_samples)\n",
    "triplet_dataloader = DataLoader(triplet_dataset, shuffle=True, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Triplet Training Loop with Loss Tracking\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "num_epochs = 100  # For demo, adjust as needed\n",
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    embedding_net.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(triplet_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        anchor, positive, negative = batch\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        anchor_out = embedding_net(anchor)\n",
    "        positive_out = embedding_net(positive)\n",
    "        negative_out = embedding_net(negative)\n",
    "\n",
    "        loss = triplet_loss_fn(anchor_out, positive_out, negative_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(triplet_dataloader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Plotting the Loss Curve\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_epochs+1), epoch_losses, marker='o', label='Training Loss')\n",
    "plt.title('Triplet Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.xticks(range(1, num_epochs+1))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Evaluation on Label Dataset with Precomputation and Visualization\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import utils.preprocessing\n",
    "from utils.preprocessing import preprocess_image\n",
    "\n",
    "model = embedding_net\n",
    "\n",
    "# Paths to directories\n",
    "dam_dir = 'data/DAM'\n",
    "test_dir = 'data/test_image_headmind'\n",
    "extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "\n",
    "# Get list of image file paths for DAM and Test\n",
    "dam_images = glob(os.path.join(dam_dir, '*.jpeg'))\n",
    "dam_images.sort()\n",
    "\n",
    "test_images = []\n",
    "for ext in extensions:\n",
    "    pattern = os.path.join(test_dir, ext)\n",
    "    test_images.extend(glob(pattern))\n",
    "test_images.sort()\n",
    "\n",
    "# Create DataFrames\n",
    "dam_df = pd.DataFrame({'image_path': dam_images})\n",
    "test_df = pd.DataFrame({'image_path': test_images})\n",
    "\n",
    "# Load labels CSV\n",
    "labels_df = pd.read_csv('labels/handmade_test_labels.csv')\n",
    "labels_dict = {}\n",
    "for _, row in labels_df.iterrows():\n",
    "    image_name = row['image'].strip()\n",
    "    references = [ref.strip() for ref in str(row['reference']).split('/') if ref.strip() and ref.strip() != '?']\n",
    "    labels_dict[image_name] = references\n",
    "\n",
    "# Define a function to predict top matches using precomputed Siamese outputs\n",
    "def find_best_match_with_precomputed(test_output, dam_outputs, top_n=1):\n",
    "    similarities = []\n",
    "    # Use Euclidean distance in the learned space for comparison\n",
    "    for dam_path, dam_out in dam_outputs.items():\n",
    "        distance = torch.norm(test_output - dam_out).item()\n",
    "        similarities.append((dam_path, distance))\n",
    "    # Lower distance implies higher similarity\n",
    "    similarities.sort(key=lambda x: x[1])\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Initialize VIT model for extracting test embeddings\n",
    "vit_model = GoogleViTModel()\n",
    "vit_model_device = device  # Use same device\n",
    "\n",
    "# Precompute Siamese network outputs for all DAM images\n",
    "model.eval()\n",
    "dam_outputs = {}\n",
    "with torch.no_grad():\n",
    "    for path, emb in dam_features.items():\n",
    "        # Convert each DAM embedding to tensor and pass through the Siamese network's first branch\n",
    "        emb_tensor = torch.tensor(emb, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        dam_out = model.forward(emb_tensor)  # Use forward_one to avoid redundant computation\n",
    "        dam_outputs[path] = dam_out.squeeze(0)  # remove batch dimension\n",
    "\n",
    "# Precompute Siamese network outputs for each test image\n",
    "test_outputs = {}  # store mapping from test image path to its Siamese output\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Precomputing test outputs\"):\n",
    "    test_path = row['image_path']\n",
    "    test_obj = preprocess_image(test_path, background_removal=\"RMBG_2\")\n",
    "    if test_obj is None:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        # Extract VIT embedding and aggregate\n",
    "        test_feat = vit_model.extract_features(test_obj)\n",
    "    test_emb = aggregate_embedding(test_feat)\n",
    "    # Pass through Siamese network's branch\n",
    "    test_tensor = torch.tensor(test_emb, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        test_out = model.forward(test_tensor)\n",
    "    test_outputs[test_path] = test_out.squeeze(0)\n",
    "\n",
    "correct_top1 = 0\n",
    "total = 0\n",
    "\n",
    "# Now evaluate using the precomputed outputs\n",
    "for test_path, test_out in tqdm(test_outputs.items(), desc=\"Evaluating\"):\n",
    "    # Find the best match using precomputed outputs\n",
    "    top_match = find_best_match_with_precomputed(test_out, dam_outputs, top_n=1)\n",
    "    predicted_path, distance = top_match[0]\n",
    "    predicted_code = os.path.basename(predicted_path).split('.')[0].split('-')[0]\n",
    "    \n",
    "    # Load original test image for plotting\n",
    "    test_img = Image.open(test_path)\n",
    "    \n",
    "    # Get true references for current test image\n",
    "    test_image_name = os.path.basename(test_path)\n",
    "    true_references = labels_dict.get(test_image_name, [])\n",
    "    \n",
    "    # Attempt to locate a true match image path from dam_features using true references\n",
    "    true_match_path = None\n",
    "    for ref in true_references:\n",
    "        for path in dam_features.keys():\n",
    "            if os.path.basename(path).startswith(ref):\n",
    "                true_match_path = path\n",
    "                break\n",
    "        if true_match_path:\n",
    "            break\n",
    "\n",
    "    if predicted_code in true_references:\n",
    "        correct_top1 += 1\n",
    "    total += 1\n",
    "    \n",
    "    # Plot test image, predicted and true match images side by side\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Display test image\n",
    "    axes[0].imshow(test_img)\n",
    "    axes[0].set_title(\"Test Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Display predicted image\n",
    "    pred_img = Image.open(predicted_path)\n",
    "    axes[1].imshow(pred_img)\n",
    "    axes[1].set_title(f\"Predicted Match\\n{os.path.basename(predicted_path)}\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Display true match image if found\n",
    "    if true_match_path:\n",
    "        true_img = Image.open(true_match_path)\n",
    "        axes[2].imshow(true_img)\n",
    "        axes[2].set_title(f\"True Match\\n{os.path.basename(true_match_path)}\")\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, \"True match not found\",\n",
    "                     horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
    "        axes[2].set_title(\"True Match\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "accuracy_top1 = correct_top1 / total if total > 0 else 0\n",
    "print(f\"Top-1 Accuracy on test set: {accuracy_top1:.2%} ({correct_top1}/{total})\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Save Model and Benchmark Results\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Save trained Siamese model\n",
    "torch.save(model.state_dict(), \".cache/models/siamese_model.pth\")\n",
    "\n",
    "# Save benchmark results\n",
    "benchmark = {\n",
    "    \"top_1_accuracy\": accuracy_top1\n",
    "}\n",
    "os.makedirs(\"benchmarks\", exist_ok=True)\n",
    "with open(os.path.join(\"benchmarks\", \"siamese_benchmark.json\"), 'w') as f:\n",
    "    json.dump(benchmark, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
