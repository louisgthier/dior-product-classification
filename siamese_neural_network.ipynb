{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install torch torchvision scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load precomputed DAM and 3D embeddings\n",
    "BASE_CACHE_DIR = '.cache'\n",
    "selected_model_key = \"GoogleViTModel\"  # adjust if needed\n",
    "BACKGROUND_REMOVAL_METHOD = 'RMBG_2'\n",
    "EMBEDDING_AGGREGRATION_METHOD = \"mean\"\n",
    "\n",
    "# Load DAM embeddings (including 3D augmentations if available)\n",
    "embeddings_file = os.path.join(BASE_CACHE_DIR, \"embeddings\", f'dam_features-{selected_model_key}-{BACKGROUND_REMOVAL_METHOD}.pkl')\n",
    "embeddings_file_3d = os.path.join(BASE_CACHE_DIR, \"embeddings\", f'dam_features-{selected_model_key}-rembg-3d.pkl')\n",
    "\n",
    "with open(embeddings_file, 'rb') as f:\n",
    "    dam_features = pickle.load(f)\n",
    "\n",
    "if os.path.exists(embeddings_file_3d):\n",
    "    with open(embeddings_file_3d, 'rb') as f:\n",
    "        dam_features_3d = pickle.load(f)\n",
    "    # Merge original and 3D features\n",
    "    dam_features.update(dam_features_3d)\n",
    "\n",
    "# Function to aggregate embeddings if needed\n",
    "def aggregate_embedding(embedding):\n",
    "    if len(embedding.shape) > 2:\n",
    "        # Assuming last dimension tokens, take mean over tokens\n",
    "        embedding = embedding.squeeze()\n",
    "        embedding = np.mean(embedding, axis=0)\n",
    "    else:\n",
    "        embedding = embedding.flatten()\n",
    "    return embedding\n",
    "\n",
    "# Process all embeddings: aggregate them for simplicity\n",
    "for key in list(dam_features.keys()):\n",
    "    dam_features[key] = aggregate_embedding(dam_features[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Network Definition\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class EmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        # Simple fully connected network to learn an embedding space\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Triplet Loss and Dataset Preparation\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# We'll use PyTorch's built-in TripletMarginLoss\n",
    "triplet_loss_fn = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, dam_features, anchor_positive_pairs, negative_samples):\n",
    "        \"\"\"\n",
    "        dam_features: dict mapping image paths to embeddings.\n",
    "        anchor_positive_pairs: list of tuples (anchor_path, positive_path).\n",
    "        negative_samples: dict mapping anchor_path to a list of negative paths.\n",
    "        \"\"\"\n",
    "        self.dam_features = dam_features\n",
    "        self.anchor_positive_pairs = anchor_positive_pairs\n",
    "        self.negative_samples = negative_samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.anchor_positive_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anchor_path, positive_path = self.anchor_positive_pairs[idx]\n",
    "        \n",
    "        # Sample one negative example for the anchor\n",
    "        negatives = self.negative_samples[anchor_path]\n",
    "        negative_path = random.choice(negatives) if negatives else None\n",
    "        \n",
    "        # Retrieve embeddings\n",
    "        anchor_emb = self.dam_features[anchor_path]\n",
    "        positive_emb = self.dam_features[positive_path]\n",
    "        negative_emb = self.dam_features[negative_path] if negative_path else positive_emb  # fallback\n",
    "        \n",
    "        return (torch.tensor(anchor_emb, dtype=torch.float32),\n",
    "                torch.tensor(positive_emb, dtype=torch.float32),\n",
    "                torch.tensor(negative_emb, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Create Triplet Data for Training\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Group 3D augmentation images by their original DAM product id\n",
    "product_to_3d = {}\n",
    "for path in dam_features.keys():\n",
    "    if \"-\" not in os.path.basename(path):\n",
    "        continue\n",
    "    basename = os.path.basename(path)\n",
    "    prod_code = basename.split(\"-\")[0]\n",
    "    product_to_3d.setdefault(prod_code, []).append(path)\n",
    "\n",
    "# Create anchor-positive pairs: pair each original DAM image with one of its 3D variants\n",
    "anchor_positive_pairs = []\n",
    "for path in list(dam_features.keys()):\n",
    "    basename = os.path.basename(path)\n",
    "    if \"-\" in basename:\n",
    "        # Skip 3D variants\n",
    "        continue\n",
    "    prod_code = basename.split(\".\")[0]\n",
    "    # Filter original images (not 3D variants)\n",
    "    if \"-\" not in basename or not basename.split(\"-\")[-1].isdigit():\n",
    "        variants = product_to_3d.get(prod_code, [])\n",
    "        # print(variants)\n",
    "        for var_path in variants:\n",
    "            if var_path != path:\n",
    "                anchor_positive_pairs.append((path, var_path))\n",
    "                # break  # use only one positive per anchor for simplicity\n",
    "\n",
    "# Create negative samples for each anchor: list of images from different products\n",
    "negative_samples = {}\n",
    "all_paths = list(dam_features.keys())\n",
    "for anchor_path, _ in anchor_positive_pairs:\n",
    "    anchor_prod = os.path.basename(anchor_path).split(\"-\")[0]\n",
    "    negatives = []\n",
    "    # Sample negatives from different products\n",
    "    for neg_path in all_paths:\n",
    "        neg_prod = os.path.basename(neg_path).split(\"-\")[0]\n",
    "        if neg_prod != anchor_prod:\n",
    "            negatives.append(neg_path)\n",
    "    negative_samples[anchor_path] = negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Initialize Network, Loss, and DataLoader for Triplet Training\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "sample_embedding = next(iter(dam_features.values()))\n",
    "input_dim = sample_embedding.shape[0]\n",
    "\n",
    "embedding_net = EmbeddingNetwork(input_dim=input_dim).to(device)\n",
    "optimizer = torch.optim.Adam(embedding_net.parameters(), lr=1e-4)\n",
    "\n",
    "# Create dataset and dataloader for triplets\n",
    "triplet_dataset = TripletDataset(dam_features, anchor_positive_pairs, negative_samples)\n",
    "triplet_dataloader = DataLoader(triplet_dataset, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Triplet Training Loop with Loss Tracking\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "num_epochs = 100\n",
    "epoch_losses = []  # List to store average loss per epoch\n",
    "\n",
    "avg_loss = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    embedding_net.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(triplet_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\"):\n",
    "        anchor, positive, negative = batch\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass anchor, positive, and negative through the network\n",
    "        anchor_out = embedding_net(anchor)\n",
    "        positive_out = embedding_net(positive)\n",
    "        negative_out = embedding_net(negative)\n",
    "\n",
    "        loss = triplet_loss_fn(anchor_out, positive_out, negative_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_loss = running_loss / len(triplet_dataloader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Plotting the Loss Curve\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_epochs+1), epoch_losses, marker='o', label='Training Loss')\n",
    "plt.title('Triplet Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.xticks(range(1, num_epochs+1))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Evaluation on Label Dataset with Precomputation and Visualization\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import utils.preprocessing\n",
    "from utils.preprocessing import preprocess_image\n",
    "from utils.models.google_vit_model import GoogleViTModel\n",
    "\n",
    "model = embedding_net\n",
    "\n",
    "# Paths to directories\n",
    "dam_dir = 'data/DAM'\n",
    "test_dir = 'data/test_image_headmind'\n",
    "extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "\n",
    "# Get list of image file paths for DAM and Test\n",
    "dam_images = glob(os.path.join(dam_dir, '*.jpeg'))\n",
    "dam_images.sort()\n",
    "\n",
    "test_images = []\n",
    "for ext in extensions:\n",
    "    pattern = os.path.join(test_dir, ext)\n",
    "    test_images.extend(glob(pattern))\n",
    "test_images.sort()\n",
    "\n",
    "# Create DataFrames\n",
    "dam_df = pd.DataFrame({'image_path': dam_images})\n",
    "test_df = pd.DataFrame({'image_path': test_images})\n",
    "\n",
    "# Load labels CSV\n",
    "labels_df = pd.read_csv('labels/handmade_test_labels.csv')\n",
    "labels_dict = {}\n",
    "for _, row in labels_df.iterrows():\n",
    "    image_name = row['image'].strip()\n",
    "    references = [ref.strip() for ref in str(row['reference']).split('/') if ref.strip() and ref.strip() != '?']\n",
    "    labels_dict[image_name] = references\n",
    "\n",
    "# Define a function to predict top matches using precomputed Siamese outputs\n",
    "def find_best_match_with_precomputed(test_output, dam_outputs, top_n=1):\n",
    "    similarities = []\n",
    "    # Use Euclidean distance in the learned space for comparison\n",
    "    for dam_path, dam_out in dam_outputs.items():\n",
    "        distance = torch.norm(test_output - dam_out).item()\n",
    "        similarities.append((dam_path, distance))\n",
    "    # Lower distance implies higher similarity\n",
    "    similarities.sort(key=lambda x: x[1])\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Initialize VIT model for extracting test embeddings\n",
    "vit_model = GoogleViTModel()\n",
    "vit_model_device = device  # Use same device\n",
    "\n",
    "# Precompute Siamese network outputs for all DAM images\n",
    "model.eval()\n",
    "dam_outputs = {}\n",
    "with torch.no_grad():\n",
    "    for path, emb in dam_features.items():\n",
    "        # Convert each DAM embedding to tensor and pass through the Siamese network's first branch\n",
    "        emb_tensor = torch.tensor(emb, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        dam_out = model.forward(emb_tensor)  # Use forward_one to avoid redundant computation\n",
    "        dam_outputs[path] = dam_out.squeeze(0)  # remove batch dimension\n",
    "\n",
    "# Precompute Siamese network outputs for each test image\n",
    "test_outputs = {}  # store mapping from test image path to its Siamese output\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Precomputing test outputs\"):\n",
    "    test_path = row['image_path']\n",
    "    test_obj = preprocess_image(test_path, background_removal=\"RMBG_2\")\n",
    "    if test_obj is None:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        # Extract VIT embedding and aggregate\n",
    "        test_feat = vit_model.extract_features(test_obj)\n",
    "    test_emb = aggregate_embedding(test_feat)\n",
    "    # Pass through Siamese network's branch\n",
    "    test_tensor = torch.tensor(test_emb, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        test_out = model.forward(test_tensor)\n",
    "    test_outputs[test_path] = test_out.squeeze(0)\n",
    "\n",
    "correct_top1 = 0\n",
    "total = 0\n",
    "\n",
    "# Now evaluate using the precomputed outputs\n",
    "for test_path, test_out in tqdm(test_outputs.items(), desc=\"Evaluating\"):\n",
    "    # Find the best match using precomputed outputs\n",
    "    top_match = find_best_match_with_precomputed(test_out, dam_outputs, top_n=1)\n",
    "    predicted_path, distance = top_match[0]\n",
    "    predicted_code = os.path.basename(predicted_path).split('.')[0].split('-')[0]\n",
    "    \n",
    "    # Load original test image for plotting\n",
    "    test_img = Image.open(test_path)\n",
    "    \n",
    "    # Get true references for current test image\n",
    "    test_image_name = os.path.basename(test_path)\n",
    "    true_references = labels_dict.get(test_image_name, [])\n",
    "    \n",
    "    # Attempt to locate a true match image path from dam_features using true references\n",
    "    true_match_path = None\n",
    "    for ref in true_references:\n",
    "        for path in dam_features.keys():\n",
    "            if os.path.basename(path).startswith(ref):\n",
    "                true_match_path = path\n",
    "                break\n",
    "        if true_match_path:\n",
    "            break\n",
    "\n",
    "    if predicted_code in true_references:\n",
    "        correct_top1 += 1\n",
    "    total += 1\n",
    "    \n",
    "    # Plot test image, predicted and true match images side by side\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Display test image\n",
    "    axes[0].imshow(test_img)\n",
    "    axes[0].set_title(\"Test Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Display predicted image\n",
    "    pred_img = Image.open(predicted_path)\n",
    "    axes[1].imshow(pred_img)\n",
    "    axes[1].set_title(f\"Predicted Match\\n{os.path.basename(predicted_path)}\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Display true match image if found\n",
    "    if true_match_path:\n",
    "        true_img = Image.open(true_match_path)\n",
    "        axes[2].imshow(true_img)\n",
    "        axes[2].set_title(f\"True Match\\n{os.path.basename(true_match_path)}\")\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, \"True match not found\",\n",
    "                     horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
    "        axes[2].set_title(\"True Match\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "accuracy_top1 = correct_top1 / total if total > 0 else 0\n",
    "print(f\"Top-1 Accuracy on test set: {accuracy_top1:.2%} ({correct_top1}/{total})\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Save Model and Benchmark Results\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Save trained Siamese model\n",
    "torch.save(model.state_dict(), \".cache/models/siamese_model.pth\")\n",
    "\n",
    "# Save benchmark results\n",
    "benchmark = {\n",
    "    \"top_1_accuracy\": accuracy_top1\n",
    "}\n",
    "os.makedirs(\"benchmarks\", exist_ok=True)\n",
    "with open(os.path.join(\"benchmarks\", \"siamese_benchmark.json\"), 'w') as f:\n",
    "    json.dump(benchmark, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
