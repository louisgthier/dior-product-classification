{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas scikit-learn Pillow numpy tqdm rembg matplotlib onnxruntime transformers torch torchvision torchaudio\n",
    "%pip install kornia timm # RMBG-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to directories\n",
    "dam_dir = 'data/DAM'\n",
    "test_dir = 'data/test_image_headmind'\n",
    "extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "\n",
    "# Get list of image file paths for DAM and Test\n",
    "dam_images = glob(os.path.join(dam_dir, '*.jpeg'))\n",
    "dam_images.sort()\n",
    "\n",
    "test_images = []\n",
    "for ext in extensions:\n",
    "    pattern = os.path.join(test_dir, ext)\n",
    "    test_images.extend(glob(pattern))\n",
    "test_images.sort()\n",
    "\n",
    "# Create DataFrames\n",
    "dam_df = pd.DataFrame({'image_path': dam_images})\n",
    "test_df = pd.DataFrame({'image_path': test_images})\n",
    "\n",
    "print(\"DAM DataFrame:\")\n",
    "print(dam_df.head())\n",
    "\n",
    "print(\"\\nTest DataFrame:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMBG-2.0 Setup\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModelForImageSegmentation\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "USE_RMBG_2 = True  # Set to False to use rembg instead of RMBG-2.0\n",
    "background_removal_method = 'RMBG_2' if USE_RMBG_2 else 'rembg'\n",
    "\n",
    "if USE_RMBG_2:\n",
    "    rmbg_model = AutoModelForImageSegmentation.from_pretrained('briaai/RMBG-2.0', trust_remote_code=True)\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    rmbg_model.to(device)\n",
    "    rmbg_model.eval()\n",
    "\n",
    "    image_size = (1024, 1024)\n",
    "    rmbg_transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "else:\n",
    "    from rembg import remove, new_session\n",
    "    import rembg\n",
    "    rembg_session = new_session('u2net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "# Define the base cache directory.\n",
    "BASE_CACHE_DIR = '.cache'\n",
    "\n",
    "def compute_image_hash(image: Image.Image) -> str:\n",
    "    \"\"\"Compute a SHA256 hash for the given PIL image.\"\"\"\n",
    "    # Convert image to bytes in a consistent format (e.g., PNG) for hashing.\n",
    "    with io.BytesIO() as buffer:\n",
    "        image.save(buffer, format=\"PNG\")\n",
    "        image_bytes = buffer.getvalue()\n",
    "    return hashlib.sha256(image_bytes).hexdigest()\n",
    "\n",
    "def compute_file_hash(file_path: str) -> str:\n",
    "    \"\"\"Compute a SHA256 hash for the given file.\"\"\"\n",
    "    sha256 = hashlib.sha256()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        while chunk := f.read(8192):\n",
    "            sha256.update(chunk)\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "def remove_background(input_image: Image.Image, input_path: str = None) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Remove background from the input image using either rembg or RMBG-2.0\n",
    "    based on the configuration flag, with caching based on image hash.\n",
    "    \"\"\"\n",
    "    # Determine the subdirectory based on the method.\n",
    "    \n",
    "    # Full cache directory for the current method.\n",
    "    cache_dir = os.path.join(BASE_CACHE_DIR, background_removal_method)\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    # Compute hash for the input image.\n",
    "    image_hash = compute_file_hash(input_path)\n",
    "    # Use .png extension to support RGBA images.\n",
    "    cached_filename = f\"{image_hash}.png\"\n",
    "    cached_path = os.path.join(cache_dir, cached_filename)\n",
    "\n",
    "    # Check if a cached version exists.\n",
    "    if os.path.exists(cached_path):\n",
    "        return Image.open(cached_path)\n",
    "\n",
    "    # No cached image found; proceed with background removal.\n",
    "    if USE_RMBG_2:\n",
    "        # Prepare input for RMBG-2.0\n",
    "        input_img_resized = input_image.convert('RGB')\n",
    "        input_tensor = rmbg_transform(input_img_resized).unsqueeze(0).to(device)\n",
    "\n",
    "        # Prediction using RMBG-2.0\n",
    "        with torch.no_grad():\n",
    "            preds = rmbg_model(input_tensor)[-1].sigmoid().cpu()\n",
    "        pred = preds[0].squeeze()\n",
    "\n",
    "        # Convert prediction mask to PIL image\n",
    "        pred_pil = transforms.ToPILImage()(pred)\n",
    "\n",
    "        # Resize mask to original image size\n",
    "        mask = pred_pil.resize(input_image.size)\n",
    "\n",
    "        # Apply mask as alpha channel to original image\n",
    "        image_rgba = input_image.convert(\"RGBA\")\n",
    "        image_rgba.putalpha(mask)\n",
    "        result_image = image_rgba\n",
    "\n",
    "    else:\n",
    "        # Use rembg for background removal\n",
    "        input_converted = input_image.convert('RGB')\n",
    "        result = remove(input_converted, session=rembg_session)\n",
    "        # rembg returns a bytes-like object or PIL image; ensure it's a PIL image.\n",
    "        if isinstance(result, bytes):\n",
    "            result_image = Image.open(io.BytesIO(result))\n",
    "        else:\n",
    "            result_image = result.convert(\"RGBA\")\n",
    "\n",
    "    # Save the processed image to the cache as PNG.\n",
    "    result_image.save(cached_path, format=\"PNG\")\n",
    "\n",
    "    return result_image\n",
    "\n",
    "\n",
    "def preprocess_image(input_path: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Preprocess the input image.\n",
    "    \"\"\"\n",
    "    \n",
    "    input: Image = Image.open(input_path)\n",
    "    \n",
    "    # if has alpha channel, use it directly; otherwise, remove background\n",
    "    has_alpha = False\n",
    "    if input.mode == 'RGBA':\n",
    "        alpha = np.array(input)[:, :, 3]\n",
    "        if not np.all(alpha == 255):\n",
    "            has_alpha = True\n",
    "    if has_alpha:\n",
    "        output = input\n",
    "    else:\n",
    "        input = input.convert('RGB')\n",
    "        max_size = max(input.size)\n",
    "        scale = min(1, 1024 / max_size)\n",
    "        if scale < 1:\n",
    "            input = input.resize((int(input.width * scale), int(input.height * scale)), Image.Resampling.LANCZOS)\n",
    "        output = remove_background(input, input_path)\n",
    "    output_np = np.array(output)\n",
    "    alpha = output_np[:, :, 3]\n",
    "    bbox = np.argwhere(alpha > 0.8 * 255)\n",
    "    try:\n",
    "        bbox = np.min(bbox[:, 1]), np.min(bbox[:, 0]), np.max(bbox[:, 1]), np.max(bbox[:, 0])\n",
    "    except ValueError:\n",
    "        return None\n",
    "        \n",
    "    center = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n",
    "    size = max(bbox[2] - bbox[0], bbox[3] - bbox[1])\n",
    "    size = int(size * 1.2)\n",
    "    bbox = center[0] - size // 2, center[1] - size // 2, center[0] + size // 2, center[1] + size // 2\n",
    "    output = output.crop(bbox)  # type: ignore\n",
    "    output = output.resize((518, 518), Image.Resampling.LANCZOS)\n",
    "    output = np.array(output).astype(np.float32) / 255\n",
    "    \n",
    "    # Set every pixel with alpha less than 0.8 to (255, 255, 255)\n",
    "    output[output[:, :, 3] < 0.8] = [1, 1, 1, 0]\n",
    "    output = output[:, :, :3]\n",
    "    \n",
    "    # Remove the alpha channel\n",
    "    # output = output[:, :, :3] * output[:, :, 3:4]\n",
    "    \n",
    "    output = Image.fromarray((output * 255).astype(np.uint8))\n",
    "    return output\n",
    "\n",
    "sample_test_path = test_df['image_path'].iloc[1]\n",
    "print(f\"Sample test path: {sample_test_path}\")\n",
    "extracted_object = preprocess_image(sample_test_path)\n",
    "\n",
    "if extracted_object is None:\n",
    "    print(\"No object found in the image at path:\", sample_test_path)\n",
    "\n",
    "# Display the original and extracted object\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(Image.open(sample_test_path))\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(extracted_object)\n",
    "plt.title(\"Extracted Object\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# for i in tqdm(range(len(dam_df))):\n",
    "#     sample_test_path = dam_df['image_path'].iloc[i]\n",
    "#     extracted_object = preprocess_image(sample_test_path)\n",
    "\n",
    "#     if extracted_object is None:\n",
    "#         print(\"No object found in the image at path:\", sample_test_path)\n",
    "#         continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import importlib.util\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define constants\n",
    "BASE_EMBEDDINGS_DIR = './embeddings'  \n",
    "selected_model_key = \"dinov2\"  \n",
    "background_removal_method = \"RMBG_2\"  \n",
    "\n",
    "\n",
    "embeddings_file = os.path.join(\n",
    "    BASE_EMBEDDINGS_DIR,\n",
    "    f'dam_features_{background_removal_method}_{selected_model_key}.pkl'\n",
    ")\n",
    "\n",
    "# Dynamically import model-specific utilities\n",
    "utils_path = f\"utils/models/{selected_model_key}_model.py\"\n",
    "spec = importlib.util.spec_from_file_location(selected_model_key, utils_path)\n",
    "model_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(model_utils)\n",
    "\n",
    "# Load the model\n",
    "base_model = model_utils.load_model()\n",
    "\n",
    "# Create the embeddings directory if it doesn't exist\n",
    "os.makedirs(BASE_EMBEDDINGS_DIR, exist_ok=True)\n",
    "\n",
    "# Extract features and save embeddings\n",
    "if os.path.exists(embeddings_file):\n",
    "    with open(embeddings_file, 'rb') as f:\n",
    "        dam_features = pickle.load(f)\n",
    "    print(f\"Loaded precomputed DAM features from {embeddings_file}.\")\n",
    "else:\n",
    "    dam_features = {}\n",
    "    for idx, row in tqdm(dam_df.iterrows(), total=len(dam_df), desc=\"Extracting DAM features\"):\n",
    "        img_path = row['image_path']\n",
    "        img = preprocess_image(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        feat = model_utils.extract_features(img, base_model)\n",
    "        dam_features[img_path] = feat\n",
    "    with open(embeddings_file, 'wb') as f:\n",
    "        pickle.dump(dam_features, f)\n",
    "    print(f\"Extracted features for DAM images and saved to {embeddings_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import pickle\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing import image as keras_image\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings\n",
    "\n",
    "# # For ViT integration\n",
    "# import torch\n",
    "# from transformers import ViTImageProcessor, ViTModel\n",
    "\n",
    "# # Define configurations for different models\n",
    "# model_configs = {\n",
    "#     \"resnet50\": {\n",
    "#         \"type\": \"tf\",\n",
    "#         \"constructor\": tf.keras.applications.ResNet50,\n",
    "#         \"weights\": \"imagenet\",\n",
    "#         \"include_top\": False,\n",
    "#         \"pooling\": \"avg\",\n",
    "#         \"target_size\": (224, 224),\n",
    "#         \"preprocess_func\": tf.keras.applications.resnet50.preprocess_input\n",
    "#     },\n",
    "#     \"vit\": {\n",
    "#         \"type\": \"pt\",\n",
    "#         \"model_name\": \"google/vit-base-patch16-224-in21k\",\n",
    "#         \"target_size\": (224, 224)  # ViT expects 224x224 images\n",
    "#     },\n",
    "#     \"clip\": {\n",
    "#         \"type\": \"pt\",\n",
    "#         \"model_name\": \"openai/clip-vit-base-patch32\",\n",
    "#         \"target_size\": (224, 224)\n",
    "#     },\n",
    "#     \"dinov2\": {\n",
    "#         \"type\": \"pt\",\n",
    "#         \"model_name\": \"facebook/dinov2-base\",\n",
    "#         \"target_size\": (224, 224)\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Select the desired model by key\n",
    "# selected_model_key = \"vit\"  # Change to \"resnet50\" or \"vit\" as needed\n",
    "\n",
    "# # Retrieve configuration\n",
    "# config = model_configs[selected_model_key]\n",
    "\n",
    "# # Initialize model based on type\n",
    "# if config[\"type\"] == \"tf\":\n",
    "#     base_model = config[\"constructor\"](\n",
    "#         weights=config[\"weights\"],\n",
    "#         include_top=config[\"include_top\"],\n",
    "#         pooling=config[\"pooling\"]\n",
    "#     )\n",
    "# elif config[\"type\"] == \"pt\":\n",
    "#     # Load ViT processor and model\n",
    "#     processor = ViTImageProcessor.from_pretrained(config[\"model_name\"])\n",
    "#     base_model = ViTModel.from_pretrained(config[\"model_name\"]).to(device)\n",
    "# else:\n",
    "#     raise ValueError(\"Unsupported model type\")\n",
    "\n",
    "# # Define embeddings file path\n",
    "# embeddings_file = os.path.join(BASE_CACHE_DIR, \"embeddings\", f'dam_features_{background_removal_method}_{selected_model_key}.pkl')\n",
    "\n",
    "# def preprocess_for_model(pil_image):\n",
    "#     target_size = config[\"target_size\"]\n",
    "#     if config[\"type\"] == \"tf\":\n",
    "#         # Preprocess for TensorFlow models like ResNet50\n",
    "#         image_array = keras_image.img_to_array(pil_image)\n",
    "#         img_resized = keras_image.smart_resize(image_array, target_size)\n",
    "#         x = keras_image.img_to_array(img_resized)\n",
    "#         x = np.expand_dims(x, axis=0)\n",
    "#         x = config[\"preprocess_func\"](x)\n",
    "#         return x\n",
    "#     elif config[\"type\"] == \"pt\":\n",
    "#         # Preprocess for ViT using its processor\n",
    "        \n",
    "#         # Plot the original image\n",
    "#         # plt.imshow(pil_image)\n",
    "#         # plt.axis('off')\n",
    "#         # plt.show()\n",
    "        \n",
    "#         inputs = processor(images=pil_image, return_tensors=\"pt\")\n",
    "        \n",
    "#         # Plot the processed image\n",
    "#         # plt.imshow((inputs.pixel_values[0].permute(1, 2, 0) + 1) / 2.0)\n",
    "#         # plt.axis('off')\n",
    "#         # plt.show()\n",
    "        \n",
    "#         return inputs\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported model type in preprocessing\")\n",
    "\n",
    "# def extract_features(pil_image):\n",
    "#     if config[\"type\"] == \"tf\":\n",
    "#         preprocessed = preprocess_for_model(pil_image)\n",
    "#         features = base_model.predict(preprocessed, verbose=0)\n",
    "#         return features.flatten()\n",
    "#     elif config[\"type\"] == \"pt\":\n",
    "#         # For ViT, use torch.no_grad() for inference\n",
    "#         inputs = preprocess_for_model(pil_image).to(device)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             outputs = base_model(**inputs)\n",
    "#         # Get last hidden state and perform global average pooling\n",
    "#         last_hidden_states = outputs.last_hidden_state  # shape: (1, sequence_length, hidden_size)\n",
    "#         # Exclude class token if present, then mean pool over tokens\n",
    "#         # For base ViT, first token is CLS, so we use all tokens except CLS for pooling\n",
    "#         token_embeddings = last_hidden_states[:, 1:, :]  # shape: (1, seq_len-1, hidden_size)\n",
    "#         pooled_embedding = token_embeddings.mean(dim=1)   # shape: (1, hidden_size)\n",
    "#         return pooled_embedding.cpu().numpy().flatten()\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported model type in feature extraction\")\n",
    "    \n",
    "# # Assuming dam_df DataFrame is defined\n",
    "\n",
    "# if os.path.exists(embeddings_file):\n",
    "#     with open(embeddings_file, 'rb') as f:\n",
    "#         dam_features = pickle.load(f)\n",
    "#     print(f\"Loaded precomputed DAM features from {embeddings_file}.\")\n",
    "# else:\n",
    "#     dam_features = {}\n",
    "#     for idx, row in tqdm(dam_df.iterrows(), total=len(dam_df), desc=\"Extracting DAM features\"):\n",
    "#         img_path = row['image_path']\n",
    "#         # Convert BGR to RGB for consistency\n",
    "#         img = preprocess_image(img_path)\n",
    "#         if img is None:\n",
    "#             # Skip if no object found\n",
    "#             continue\n",
    "#         feat = extract_features(img)\n",
    "#         dam_features[img_path] = feat\n",
    "#     os.makedirs(os.path.dirname(embeddings_file), exist_ok=True)\n",
    "#     with open(embeddings_file, 'wb') as f:\n",
    "#         pickle.dump(dam_features, f)\n",
    "#     print(f\"Extracted features for DAM images and saved to {embeddings_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the labels CSV into a DataFrame\n",
    "labels_df = pd.read_csv('labels/handmade_test_labels.csv')\n",
    "\n",
    "# Create a dictionary mapping each test image filename to a list of reference labels\n",
    "labels_dict = {}\n",
    "for _, row in labels_df.iterrows():\n",
    "    image_name = row['image'].strip()\n",
    "    # Split the reference column by '/' to handle multiple labels, remove empty strings and spaces\n",
    "    references = [ref.strip() for ref in str(row['reference']).split('/') if ref.strip() and ref.strip() != '?']\n",
    "    labels_dict[image_name] = references\n",
    "\n",
    "# For debugging, you can print a sample of the dictionary\n",
    "print(dict(list(labels_dict.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize accuracy counters\n",
    "total_queries = 0\n",
    "correct_top1 = 0\n",
    "correct_top3 = 0\n",
    "correct_top5 = 0\n",
    "\n",
    "# This dictionary will store the index of the correct match in each query if found\n",
    "correct_match_indices = {}\n",
    "\n",
    "# Set the desired number of top matches to display\n",
    "top_n = 10\n",
    "max_height = 5\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing test images\"):\n",
    "    test_path = row['image_path']\n",
    "    \n",
    "    # if not \"image-20210928-103146-c8d2fedb.jpg\".lower() in test_path.lower():\n",
    "    #     continue\n",
    "    \n",
    "    total_queries += 1\n",
    "    t = time.time()\n",
    "    \n",
    "    # Preprocess and extract object from test image\n",
    "    # Adjust this line to use your actual extraction method\n",
    "    # test_obj = original_image\n",
    "    test_obj = preprocess_image(test_path)\n",
    "    \n",
    "\n",
    "    print(f\"Preprocessing time: {time.time() - t:.4f} seconds\")\n",
    "    t = time.time()\n",
    "    \n",
    "    # Extract features for the test object\n",
    "    test_feat = model_utils.extract_features(test_obj, base_model)  # Utilise la fonction spécifique au modèle\n",
    "    \n",
    "    print(f\"Feature extraction time: {time.time() - t:.4f} seconds\")\n",
    "    t = time.time()\n",
    "    \n",
    "    # Compute similarities with all DAM features\n",
    "    similarities = []\n",
    "    for dam_path, dam_feat in dam_features.items():\n",
    "        sim = cosine_similarity([test_feat], [dam_feat])[0][0]\n",
    "        similarities.append((dam_path, sim))\n",
    "    \n",
    "    # Sort and retrieve the top `top_n` matches\n",
    "    top_matches = sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'test_image': test_path,\n",
    "        'top_matches': top_matches\n",
    "    })\n",
    "    \n",
    "    # Retrieve true reference labels for the current test image\n",
    "    test_image_name = os.path.basename(test_path)\n",
    "    true_references = labels_dict.get(test_image_name, [])\n",
    "    \n",
    "    # Check positions of correct matches in the top results\n",
    "    found_indices = []  # list to store indices of correct matches found in top_n\n",
    "    for i, (match_path, sim_score) in enumerate(top_matches):\n",
    "        predicted_code = os.path.splitext(os.path.basename(match_path))[0]\n",
    "        if predicted_code in true_references:\n",
    "            found_indices.append(i)  # store zero-based index of the correct match\n",
    "\n",
    "    # If we found any correct match, record the smallest index (closest to top)\n",
    "    if found_indices:\n",
    "        correct_index = min(found_indices)\n",
    "        correct_match_indices[test_image_name] = correct_index\n",
    "        # Update counters for top-1, top-3, and top-5\n",
    "        if correct_index < 1:\n",
    "            correct_top1 += 1\n",
    "        if correct_index < 3:\n",
    "            correct_top3 += 1\n",
    "        if correct_index < 5:\n",
    "            correct_top5 += 1\n",
    "    \n",
    "    # Determine number of columns for grid specification\n",
    "    ncols = min(max(2, top_n), 5)  # Ensure at least 2 columns for original and extracted images\n",
    "    nrows = math.ceil(top_n / ncols) + 1  # Add 1 for the original and extracted images\n",
    "    \n",
    "    # Create subplots with 2 rows and `ncols` columns\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(4 * ncols, max_height * nrows))\n",
    "    gs = fig.add_gridspec(nrows, ncols)\n",
    "    \n",
    "    # Subplot for original image (row 0, col 0)\n",
    "    ax_orig = fig.add_subplot(gs[0, 0])\n",
    "    ax_orig.imshow(Image.open(test_path))\n",
    "    ax_orig.set_title(\"Original Image\")\n",
    "    ax_orig.axis('off')\n",
    "    \n",
    "    # Subplot for extracted object (row 0, col 1)\n",
    "    ax_ext = fig.add_subplot(gs[0, 1])\n",
    "    ax_ext.imshow(test_obj)\n",
    "    ax_ext.set_title(\"Extracted Object\")\n",
    "    ax_ext.axis('off')\n",
    "    \n",
    "    # Leave remaining cells in the first row empty if any\n",
    "    for col in range(2, ncols):\n",
    "        ax_empty = fig.add_subplot(gs[0, col])\n",
    "        ax_empty.axis('off')\n",
    "    \n",
    "    # Display top `top_n` matches in the second row\n",
    "    for i, (match_path, sim_score) in enumerate(top_matches):\n",
    "        ax = fig.add_subplot(gs[i // ncols + 1, i % ncols])\n",
    "        img_match = Image.open(match_path)\n",
    "        ax.imshow(img_match)\n",
    "        ax.set_title(f\"Match {i+1}\\nSim: {sim_score:.4f}\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # After processing all test images, compute and display overall accuracy metrics\n",
    "    if total_queries > 0:\n",
    "        accuracy_top1 = correct_top1 / total_queries\n",
    "        accuracy_top3 = correct_top3 / total_queries\n",
    "        accuracy_top5 = correct_top5 / total_queries\n",
    "\n",
    "        print(f\"Total queries processed: {total_queries}\")\n",
    "        print(f\"Top-1 Accuracy: {accuracy_top1:.2%} ({correct_top1} correct)\")\n",
    "        print(f\"Top-3 Accuracy: {accuracy_top3:.2%} ({correct_top3} correct)\")\n",
    "        print(f\"Top-5 Accuracy: {accuracy_top5:.2%} ({correct_top5} correct)\")\n",
    "    else:\n",
    "        print(\"No queries were processed.\")\n",
    "\n",
    "    # Optionally print the indices where correct matches were found for each image\n",
    "    if test_image_name in correct_match_indices:\n",
    "        print(f\"Correct match found at top-{correct_match_indices[test_image_name]+1}\")\n",
    "        \n",
    "    # Display match information in console\n",
    "    tqdm.write(f\"Processed {test_path} -> Top {top_n} matches:\")\n",
    "    for match in top_matches:\n",
    "        tqdm.write(f\"\\tMatch: {match[0]}, Similarity: {match[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
