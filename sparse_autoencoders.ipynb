{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sparse Autoencoders (SAEs) for Enhanced Embedding Refinement\n",
    "\n",
    "This notebook trains Sparse Autoencoders (SAEs) to refine embeddings from a Vision Transformer (ViT). The goal is to improve the separation of similar product embeddings by selecting the most relevant SAE features. The process includes data preparation, model training, feature selection, and evaluation with visualization.\n",
    "\n",
    "---\n",
    "\n",
    "## Install Required Packages\n",
    "\n",
    "```python\n",
    "%pip install torch torchvision scikit-learn tqdm faiss-cpu matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import faiss\n",
    "from PIL import Image\n",
    "\n",
    "from utils.models.google_vit_model import GoogleViTModel\n",
    "from utils.preprocessing import preprocess_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precomputed DAM and 3D embeddings\n",
    "BASE_CACHE_DIR = '.cache'\n",
    "selected_model_key = GoogleViTModel().model_name  # adjust if needed\n",
    "BACKGROUND_REMOVAL_METHOD = 'RMBG_2'\n",
    "EMBEDDING_AGGREGRATION_METHOD = \"mean\"\n",
    "\n",
    "# Load DAM embeddings (including 3D augmentations if available)\n",
    "embeddings_file = os.path.join(BASE_CACHE_DIR, \"embeddings\", f'dam_features-{selected_model_key}-{BACKGROUND_REMOVAL_METHOD}.pkl')\n",
    "embeddings_file_3d = os.path.join(BASE_CACHE_DIR, \"embeddings\", f'dam_features-{selected_model_key}-rembg-3d.pkl')\n",
    "\n",
    "with open(embeddings_file, 'rb') as f:\n",
    "    dam_features = pickle.load(f)\n",
    "    \n",
    "# Replace each item with 8 duplicates with -X at the end of the key\n",
    "for key in list(dam_features.keys()):\n",
    "    for i in range(1, 8):\n",
    "        dam_features[f\"{key}-dup-{i}\"] = dam_features[key]\n",
    "    del dam_features[key]\n",
    "\n",
    "\n",
    "if os.path.exists(embeddings_file_3d):\n",
    "    with open(embeddings_file_3d, 'rb') as f:\n",
    "        dam_features_3d = pickle.load(f)\n",
    "    # Merge original and 3D features\n",
    "    dam_features.update(dam_features_3d)\n",
    "\n",
    "# Function to aggregate embeddings if needed\n",
    "def aggregate_embedding(embedding):\n",
    "    if len(embedding.shape) >= 2:\n",
    "        # Assuming last dimension tokens, take mean over tokens\n",
    "        embedding = embedding.squeeze()\n",
    "        # Reshape into 2D matrix\n",
    "        # embedding = embedding[1:]  # Remove CLS token\n",
    "        # embedding = embedding.reshape(-1, 14, 14)\n",
    "        \n",
    "        embedding = embedding.mean(axis=0)\n",
    "        \n",
    "        # embedding = embedding.max(axis=0)\n",
    "        \n",
    "        # embedding = embedding[0, :]\n",
    "    else:\n",
    "        embedding = embedding.flatten()\n",
    "    return embedding\n",
    "\n",
    "# Process all embeddings: aggregate them for simplicity\n",
    "for key in list(dam_features.keys()):\n",
    "    dam_features[key] = aggregate_embedding(dam_features[key])\n",
    "    # print(dam_features[key].shape)\n",
    "    \n",
    "print(f\"Loaded {len(dam_features)} DAM embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, sparsity_lambda=1e-3):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Assuming input embeddings are normalized\n",
    "        )\n",
    "        self.sparsity_lambda = sparsity_lambda\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "    def sparsity_loss(self, encoded):\n",
    "        # L1 sparsity penalty\n",
    "        return self.sparsity_lambda * torch.mean(torch.abs(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Assuming input embeddings are normalized\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, latent_dim=256):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(hidden_dim // 2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim // 2, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()  # Assuming input embeddings are normalized\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)  # Sample from standard normal\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, dam_features):\n",
    "        \"\"\"\n",
    "        dam_features: dict mapping image paths to aggregated embeddings.\n",
    "        \"\"\"\n",
    "        self.paths = list(dam_features.keys())\n",
    "        self.embeddings = np.array([dam_features[p].flatten() for p in self.paths], dtype=np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding = self.embeddings[idx]\n",
    "        return torch.tensor(embedding, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample embedding to determine input dimension\n",
    "sample_embedding = next(iter(dam_features.values()))\n",
    "input_dim = sample_embedding.flatten().shape[0]\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "\n",
    "# Model Selection\n",
    "model_type = 'AE'  # Options: 'SAE', 'AE', 'VAE'\n",
    "\n",
    "# Parameters (you can adjust these as needed)\n",
    "input_dim = sample_embedding.flatten().shape[0]\n",
    "hidden_dim = 256\n",
    "latent_dim = 256  # Only used for VAE\n",
    "sparsity_lambda = 1e-3  # Only used for SAE\n",
    "\n",
    "# Instantiate the selected model\n",
    "if model_type == 'SAE':\n",
    "    autoencoder = SparseAutoencoder(input_dim=input_dim, hidden_dim=hidden_dim, sparsity_lambda=sparsity_lambda).to(device)\n",
    "elif model_type == 'AE':\n",
    "    autoencoder = Autoencoder(input_dim=input_dim, hidden_dim=hidden_dim).to(device)\n",
    "elif model_type == 'VAE':\n",
    "    autoencoder = VariationalAutoencoder(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=latent_dim).to(device)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
    "    \n",
    "print(f\"Selected model: {model_type}\")\n",
    "\n",
    "# Define loss functions\n",
    "reconstruction_loss_fn = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "embedding_dataset = EmbeddingDataset(dam_features)\n",
    "embedding_dataloader = DataLoader(embedding_dataset, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50  # Adjust as needed\n",
    "epoch_losses = []\n",
    "\n",
    "autoencoder.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(embedding_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if model_type == 'VAE':\n",
    "            reconstructed, mu, logvar = autoencoder(batch)\n",
    "            # Reconstruction loss\n",
    "            loss_recon = reconstruction_loss_fn(reconstructed, batch)\n",
    "            # KL Divergence loss\n",
    "            kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            loss = loss_recon + kl_loss\n",
    "        elif model_type == 'SAE':\n",
    "            reconstructed, encoded = autoencoder(batch)\n",
    "            loss_recon = reconstruction_loss_fn(reconstructed, batch)\n",
    "            loss_sparsity = autoencoder.sparsity_loss(encoded)\n",
    "            loss = loss_recon + loss_sparsity\n",
    "        else:  # Regular AE\n",
    "            reconstructed, encoded = autoencoder(batch)\n",
    "            loss_recon = reconstruction_loss_fn(reconstructed, batch)\n",
    "            loss = loss_recon\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(embedding_dataloader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, min(num_epochs+1, len(epoch_losses)+1)), epoch_losses, marker='o', label='Training Loss')\n",
    "plt.title('Autoencoder Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(range(1, num_epochs+1))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract encoded features for all embeddings\n",
    "# Extract encoded features for all embeddings\n",
    "autoencoder.eval()\n",
    "encoded_features = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for path, emb in tqdm(dam_features.items(), desc=\"Encoding embeddings\"):\n",
    "        emb_tensor = torch.tensor(emb.flatten(), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        if model_type == 'VAE':\n",
    "            reconstructed, mu, logvar = autoencoder(emb_tensor)\n",
    "            encoded = mu  # Use the mean as the encoded feature\n",
    "            # Optionally, you can store logvar if needed for analysis\n",
    "            # encoded_features[path] = {\"mu\": mu.cpu().squeeze(0).numpy(), \"logvar\": logvar.cpu().squeeze(0).numpy()}\n",
    "        else:\n",
    "            reconstructed, encoded = autoencoder(emb_tensor)\n",
    "            # encoded = encoded.cpu().squeeze(0).numpy()\n",
    "        \n",
    "        encoded_features[path] = encoded.cpu().squeeze(0).numpy()\n",
    "\n",
    "# Convert to numpy array for further analysis\n",
    "encoded_matrix = np.array(list(encoded_features.values()))\n",
    "print(f\"Encoded features shape: {encoded_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Initialize an empty dictionary to hold the final encoded features\n",
    "selected_encoded_features = {}\n",
    "\n",
    "if model_type != 'VAE':\n",
    "    # Remove low-variance features which might be less informative\n",
    "    selector = VarianceThreshold(threshold=0.01)  # Adjust threshold as needed\n",
    "    selector.fit(encoded_matrix)\n",
    "    selected_features = selector.get_support(indices=True)\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} out of {encoded_matrix.shape[1]} features based on variance threshold.\")\n",
    "    \n",
    "    # Apply feature selection\n",
    "    encoded_matrix_selected = selector.transform(encoded_matrix)\n",
    "    print(f\"Shape after feature selection: {encoded_matrix_selected.shape}\")\n",
    "    \n",
    "    # Update the encoded_features dictionary\n",
    "    selected_encoded_features = {\n",
    "        path: encoded_matrix_selected[idx] for idx, path in enumerate(encoded_features.keys())\n",
    "    }\n",
    "else:\n",
    "    # For VAEs, skip feature selection and use all encoded features\n",
    "    print(\"Feature selection is disabled for VAEs. Using all encoded features.\")\n",
    "    \n",
    "    # No transformation needed\n",
    "    encoded_matrix_selected = encoded_matrix\n",
    "    selected_encoded_features = {\n",
    "        path: encoded_matrix_selected[idx] for idx, path in enumerate(encoded_features.keys())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions before and after selection\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Before selection\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(encoded_matrix.flatten(), bins=50, alpha=0.7, color='blue')\n",
    "plt.title('Encoded Features Distribution (All)')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# After selection\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(encoded_matrix_selected.flatten(), bins=50, alpha=0.7, color='green')\n",
    "plt.title('Encoded Features Distribution (Selected)')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to directories\n",
    "dam_dir = 'data/DAM'\n",
    "test_dir = 'data/test_image_headmind'\n",
    "extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "\n",
    "# Get list of image file paths for DAM and Test\n",
    "dam_images = glob(os.path.join(dam_dir, '*.jpeg'))\n",
    "dam_images.sort()\n",
    "\n",
    "test_images = []\n",
    "for ext in extensions:\n",
    "    pattern = os.path.join(test_dir, ext)\n",
    "    test_images.extend(glob(pattern))\n",
    "test_images.sort()\n",
    "\n",
    "# Create DataFrames\n",
    "dam_df = pd.DataFrame({'image_path': dam_images})\n",
    "test_df = pd.DataFrame({'image_path': test_images})\n",
    "\n",
    "# Load labels CSV\n",
    "labels_df = pd.read_csv('labels/handmade_test_labels.csv')\n",
    "labels_dict = {}\n",
    "for _, row in labels_df.iterrows():\n",
    "    image_name = row['image'].strip()\n",
    "    references = [ref.strip() for ref in str(row['reference']).split('/') if ref.strip() and ref.strip() != '?']\n",
    "    labels_dict[image_name] = references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute encoded features for all DAM images\n",
    "encoded_dam_features = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for path in tqdm(dam_features.keys(), desc=\"Encoding DAM embeddings\"):\n",
    "        emb = dam_features[path].flatten()\n",
    "        emb_tensor = torch.tensor(emb, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        if model_type == 'VAE':\n",
    "            reconstructed, mu, logvar = autoencoder(emb_tensor)\n",
    "            encoded_np = mu.cpu().squeeze(0).numpy()\n",
    "        else:\n",
    "            reconstructed, encoded = autoencoder(emb_tensor)\n",
    "            encoded_np = encoded.cpu().squeeze(0).numpy()\n",
    "        \n",
    "        if model_type != 'VAE':\n",
    "            # Apply feature selection\n",
    "            encoded_selected = selector.transform(encoded_np.reshape(1, -1)).squeeze(0)\n",
    "        else:\n",
    "            # Use all features\n",
    "            encoded_selected = encoded_np\n",
    "        \n",
    "        encoded_dam_features[path] = encoded_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VIT model for extracting test embeddings\n",
    "vit_model = GoogleViTModel()\n",
    "vit_model_device = device  # Use same device\n",
    "\n",
    "# Precompute encoded features for each test image\n",
    "encoded_test_features = {}\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Encoding Test embeddings\"):\n",
    "    test_path = row['image_path']\n",
    "    test_obj = preprocess_image(test_path, background_removal=\"RMBG_2\")\n",
    "    if test_obj is None:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        # Extract VIT embedding and aggregate\n",
    "        test_feat = vit_model.extract_features(test_obj)\n",
    "    test_emb = aggregate_embedding(test_feat).flatten()\n",
    "    # Pass through Autoencoder's encoder\n",
    "    test_tensor = torch.tensor(test_emb, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    if model_type == 'VAE':\n",
    "        reconstructed, mu, logvar = autoencoder(test_tensor)\n",
    "        encoded_np = mu.detach().cpu().squeeze(0).numpy()\n",
    "    else:\n",
    "        reconstructed, encoded = autoencoder(test_tensor)\n",
    "        encoded_np = encoded.detach().cpu().squeeze(0).numpy()\n",
    "    \n",
    "    if model_type != 'VAE':\n",
    "        # Apply feature selection\n",
    "        encoded_selected = selector.transform(encoded_np.reshape(1, -1)).squeeze(0)\n",
    "    else:\n",
    "        # Use all features\n",
    "        encoded_selected = encoded_np\n",
    "    \n",
    "    encoded_test_features[test_path] = encoded_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert encoded DAM features to numpy array\n",
    "dam_paths = list(encoded_dam_features.keys())\n",
    "dam_encoded_matrix = np.array([encoded_dam_features[p] for p in dam_paths], dtype=np.float32)\n",
    "\n",
    "# Build FAISS index\n",
    "dim = dam_encoded_matrix.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)  # Using Euclidean distance\n",
    "index.add(dam_encoded_matrix)\n",
    "print(f\"FAISS index built with {index.ntotal} vectors, dimension={dim}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_match(test_feature, dam_features, index, dam_paths, top_n=1):\n",
    "    \"\"\"\n",
    "    Find the best matching DAM image for a given test feature.\n",
    "    \"\"\"\n",
    "    test_feature = np.expand_dims(test_feature, axis=0).astype(np.float32)\n",
    "    distances, indices = index.search(test_feature, top_n)\n",
    "    best_matches = [(dam_paths[idx], distances[0][i]) for i, idx in enumerate(indices[0])]\n",
    "    return best_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_top1 = 0\n",
    "total = 0\n",
    "\n",
    "for test_path, test_feat in tqdm(encoded_test_features.items(), desc=\"Evaluating\"):\n",
    "    test_path = test_path.split(\"-dup\")[0]\n",
    "    top_matches = find_best_match(test_feat, encoded_dam_features, index, dam_paths, top_n=1)\n",
    "    predicted_path, distance = top_matches[0]\n",
    "    predicted_path = predicted_path.split(\"-dup\")[0]\n",
    "    predicted_code = os.path.basename(predicted_path).split('.')[0].split('-')[0]\n",
    "    \n",
    "    # Get true references for current test image\n",
    "    test_image_name = os.path.basename(test_path)\n",
    "    true_references = labels_dict.get(test_image_name, [])\n",
    "    \n",
    "    if predicted_code in true_references:\n",
    "        correct_top1 += 1\n",
    "    total += 1\n",
    "    \n",
    "    # Plot test image, predicted and true match images side by side\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Display test image\n",
    "    try:\n",
    "        test_img = Image.open(test_path)\n",
    "        axes[0].imshow(test_img)\n",
    "    except:\n",
    "        axes[0].text(0.5, 0.5, \"Image not found\",\n",
    "                     horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
    "    axes[0].set_title(\"Test Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Display predicted image\n",
    "    try:\n",
    "        pred_img = Image.open(predicted_path)\n",
    "        axes[1].imshow(pred_img)\n",
    "    except:\n",
    "        axes[1].text(0.5, 0.5, \"Image not found\",\n",
    "                     horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
    "    axes[1].set_title(f\"Predicted Match\\n{os.path.basename(predicted_path)}\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Display true match image if found\n",
    "    true_match_path = None\n",
    "    for ref in true_references:\n",
    "        for path in dam_features.keys():\n",
    "            if os.path.basename(path).startswith(ref):\n",
    "                true_match_path = path\n",
    "                break\n",
    "        if true_match_path:\n",
    "            break\n",
    "    \n",
    "    true_match_path = true_match_path.split(\"-dup\")[0] if true_match_path else None\n",
    "    \n",
    "    if true_match_path:\n",
    "        try:\n",
    "            true_img = Image.open(true_match_path)\n",
    "            axes[2].imshow(true_img)\n",
    "        except:\n",
    "            axes[2].text(0.5, 0.5, \"Image not found\",\n",
    "                         horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, \"True match not found\",\n",
    "                     horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
    "    axes[2].set_title(\"True Match\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "accuracy_top1 = correct_top1 / total if total > 0 else 0\n",
    "print(f\"Top-1 Accuracy on test set: {accuracy_top1:.2%} ({correct_top1}/{total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained Autoencoder model\n",
    "model_save_path = f\".cache/models/{model_type.lower()}_autoencoder.pth\"\n",
    "torch.save(autoencoder.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save benchmark results\n",
    "benchmark = {\n",
    "    \"model_type\": model_type,\n",
    "    \"top_1_accuracy\": accuracy_top1\n",
    "}\n",
    "benchmark_save_path = os.path.join(\"benchmarks\", f\"{model_type.lower()}_benchmark.json\")\n",
    "with open(benchmark_save_path, 'w') as f:\n",
    "    json.dump(benchmark, f)\n",
    "print(f\"Benchmark results saved to {benchmark_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
