{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Google Vision Transformer (ViT) for Custom Classification\n",
    "\n",
    "This notebook demonstrates how to fine-tune a pre-trained Google Vision Transformer (ViT) model for image classification on a custom dataset. Each class corresponds to a unique image from the DAM dataset, identified by its `id.jpg`, and includes 8 associated 3D images (`id-X.png`). The model is trained on the full DAM dataset and evaluated on a separate test set with labels provided in a handmade CSV file.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Installation of Required Libraries](#installation)\n",
    "2. [Importing Libraries](#importing-libraries)\n",
    "3. [Setting Up Paths and Device](#paths-and-device)\n",
    "4. [Loading and Preparing the Dataset](#loading-dataset)\n",
    "5. [Preprocessing Images](#preprocessing)\n",
    "6. [Preparing the Hugging Face Dataset](#hugging-face-dataset)\n",
    "7. [Defining the Model](#defining-model)\n",
    "8. [Training Arguments](#training-arguments)\n",
    "9. [Metrics Calculation](#metrics)\n",
    "10. [Training the Model](#training)\n",
    "11. [Evaluating the Model](#evaluation)\n",
    "12. [Saving the Fine-Tuned Model](#saving-model)\n",
    "13. [Conclusion](#conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"installation\"></a>\n",
    "## 1. Installation of Required Libraries\n",
    "\n",
    "Ensure that all necessary libraries are installed. Run the following cell to install them.\n",
    "\n",
    "```python\n",
    "# Install necessary libraries\n",
    "%pip install transformers datasets torch torchvision pandas scikit-learn Pillow numpy tqdm rembg matplotlib faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "import faiss  # For similarity search (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DAM_DIR = 'data/DAM'\n",
    "TEST_DIR = 'data/test_image_headmind'\n",
    "LABELS_CSV = 'labels/handmade_test_labels.csv'\n",
    "TRELLIS_DIR = '.cache/TRELLIS'\n",
    "\n",
    "# Supported image extensions\n",
    "EXTENSIONS = ['*.jpg', '*.jpeg', '*.png']\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of DAM original image file paths (id.jpg)\n",
    "dam_original_images = glob(os.path.join(DAM_DIR, '*.jpg')) + glob(os.path.join(DAM_DIR, '*.jpeg')) + glob(os.path.join(DAM_DIR, '*.png'))\n",
    "dam_original_images.sort()\n",
    "\n",
    "# Function to retrieve 3D augmented image paths for a given class ID\n",
    "def get_3d_augmented_paths(class_id):\n",
    "    augmented_paths = []\n",
    "    for i in range(1, 9):  # i from 1 to 8\n",
    "        augmented_path = os.path.join(TRELLIS_DIR, class_id, f\"{class_id}-{i}.png\")\n",
    "        if os.path.exists(augmented_path):\n",
    "            augmented_paths.append(augmented_path)\n",
    "    return augmented_paths\n",
    "\n",
    "# Create a list to hold all image paths (original + augmented)\n",
    "all_dam_images = []\n",
    "labels = []\n",
    "\n",
    "for img_path in dam_original_images:\n",
    "    class_id = os.path.splitext(os.path.basename(img_path))[0]  # Remove extension\n",
    "    all_dam_images.append(img_path)\n",
    "    labels.append(class_id)\n",
    "    \n",
    "    # Add 3D augmented images\n",
    "    augmented_paths = get_3d_augmented_paths(class_id)\n",
    "    all_dam_images.extend(augmented_paths)\n",
    "    labels.extend([class_id] * len(augmented_paths))  # Same label for augmented images\n",
    "\n",
    "# Create DataFrame\n",
    "dam_df = pd.DataFrame({\n",
    "    'image_path': all_dam_images,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "# Get list of Test image file paths\n",
    "test_images = []\n",
    "for ext in EXTENSIONS:\n",
    "    pattern = os.path.join(TEST_DIR, ext)\n",
    "    test_images.extend(glob(pattern))\n",
    "test_images.sort()\n",
    "\n",
    "# Create Test DataFrame\n",
    "test_df = pd.DataFrame({'image_path': test_images})\n",
    "\n",
    "print(\"DAM DataFrame:\")\n",
    "print(dam_df.head())\n",
    "\n",
    "print(\"\\nTest DataFrame:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import preprocess_image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract class labels from DAM image filenames (using the id)\n",
    "def extract_label(image_path):\n",
    "    filename = os.path.basename(image_path)\n",
    "    label = filename.split('.')[0]  # Assumes label is the part before the first dot\n",
    "    label = label.split('-')[0]  # Remove augmented image number if present\n",
    "    return label\n",
    "\n",
    "dam_df['label'] = dam_df['image_path'].apply(extract_label)\n",
    "\n",
    "# Since each class has multiple images (1 original + 8 3D images), ensure all are included\n",
    "# No need to split into train/validation here as we'll use the full dataset for training\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "hf_dataset = HFDataset.from_pandas(dam_df)\n",
    "\n",
    "# Split the dataset into training and validation sets if desired\n",
    "# For simplicity, we'll proceed without a validation split here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the image processor\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "def transform(example_batch):\n",
    "    # Preprocess images\n",
    "    images = [preprocess_image(img_path, background_removal=\"rembg\") for img_path in example_batch['image_path']]\n",
    "    images = [img for img in images if img is not None]\n",
    "\n",
    "    if len(images) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Apply processor to images\n",
    "    inputs = processor(images=images, return_tensors='pt')\n",
    "    \n",
    "    # Extract labels\n",
    "    labels = [example_batch['label'][i] for i in range(len(example_batch['label'])) if preprocess_image(example_batch['image_path'][i], background_removal=\"rembg\") is not None]\n",
    "    \n",
    "    # Ensure labels align with processed images\n",
    "    if len(labels) != len(inputs['pixel_values']):\n",
    "        min_len = min(len(labels), len(inputs['pixel_values']))\n",
    "        labels = labels[:min_len]\n",
    "        inputs['pixel_values'] = inputs['pixel_values'][:min_len]\n",
    "    \n",
    "    # Create a label mapping if not already defined\n",
    "    unique_labels = sorted(list(set(labels)))\n",
    "    label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "    \n",
    "    # Map labels to IDs\n",
    "    inputs['labels'] = torch.tensor([label2id[label] for label in labels], dtype=torch.long)\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# Apply the transformation\n",
    "prepared_dataset = hf_dataset.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of classes\n",
    "num_classes = len(dam_df['label'].unique())\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Create label mappings\n",
    "labels = sorted(dam_df['label'].unique())\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Initialize the model with a classification head\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224-in21k',\n",
    "    num_labels=num_classes,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./vit-finetuned',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-4,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "# Load the accuracy metric\n",
    "metric = load('accuracy')\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=prepared_dataset,\n",
    "    eval_dataset=prepared_dataset,  # For demonstration; ideally, use a separate validation set\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this variable to True to enable result display\n",
    "DISPLAY_RESULTS = True\n",
    "\n",
    "# Load the test labels\n",
    "labels_df = pd.read_csv(LABELS_CSV)\n",
    "\n",
    "# Create a dictionary mapping each test image filename to its labels\n",
    "labels_dict = {}\n",
    "for _, row in labels_df.iterrows():\n",
    "    image_name = row['image'].strip()\n",
    "    references = [ref.strip() for ref in str(row['reference']).split('/') if ref.strip() and ref.strip() != '?']\n",
    "    labels_dict[image_name] = references\n",
    "\n",
    "# Function to extract label from test image path\n",
    "# def extract_test_label(image_path):\n",
    "#     filename = os.path.basename(image_path)\n",
    "#     label = filename.split('.')[0]  # Assumes label is the part before the first dot\n",
    "#     return label\n",
    "\n",
    "# Update test DataFrame with labels\n",
    "test_df['label'] = test_df['image_path'].apply(extract_label)\n",
    "\n",
    "# Convert Test DataFrame to Hugging Face Dataset\n",
    "test_hf_dataset = HFDataset.from_pandas(test_df)\n",
    "\n",
    "# Define transformation for test dataset\n",
    "def test_transform(example_batch):\n",
    "    # Preprocess images\n",
    "    images = [preprocess_image(img_path, background_removal=\"RMBG_2\") for img_path in example_batch['image_path']]\n",
    "    images = [img for img in images if img is not None]\n",
    "\n",
    "    if len(images) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Apply processor to images\n",
    "    inputs = processor(images=images, return_tensors='pt')\n",
    "    \n",
    "    # Extract labels\n",
    "    labels = [example_batch['label'][i] for i in range(len(example_batch['label'])) if preprocess_image(example_batch['image_path'][i], background_removal=\"RMBG_2\") is not None]\n",
    "    \n",
    "    # Ensure labels align with processed images\n",
    "    if len(labels) != len(inputs['pixel_values']):\n",
    "        min_len = min(len(labels), len(inputs['pixel_values']))\n",
    "        labels = labels[:min_len]\n",
    "        inputs['pixel_values'] = inputs['pixel_values'][:min_len]\n",
    "    \n",
    "    # Map labels to IDs using the same mapping as training\n",
    "    inputs['labels'] = torch.tensor([label2id.get(label, -1) for label in labels], dtype=torch.long)\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# Apply the transformation to the test dataset\n",
    "prepared_test_dataset = test_hf_dataset.with_transform(test_transform)\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate(prepared_test_dataset)\n",
    "print(\"Evaluation Results:\")\n",
    "print(results)\n",
    "\n",
    "# If DISPLAY_RESULTS is True, display each test image with expected and predicted DAM images\n",
    "if DISPLAY_RESULTS:\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    # Initialize the image classification pipeline\n",
    "    classifier = pipeline(\"image-classification\", model=model, image_processor=processor, device=device)\n",
    "    \n",
    "    # Create a mapping from label to original DAM image path (original image, not augmented)\n",
    "    label_to_original_path = {}\n",
    "    for _, row in dam_df.iterrows():\n",
    "        label = row['label']\n",
    "        image_path = row['image_path']\n",
    "        basename = os.path.basename(image_path)\n",
    "        # Assuming original images do not have '-X' in their filenames\n",
    "        if '-' not in basename:\n",
    "            if label not in label_to_original_path:\n",
    "                label_to_original_path[label] = image_path\n",
    "    \n",
    "    # Iterate over each test image\n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Displaying Results\"):\n",
    "        test_image_path = row['image_path']\n",
    "        test_image_name = os.path.basename(test_image_path)\n",
    "        expected_labels = labels_dict.get(test_image_name, [])\n",
    "        \n",
    "        # Get expected DAM image paths\n",
    "        expected_dam_paths = [label_to_original_path.get(label, None) for label in expected_labels]\n",
    "        expected_dam_paths = [path for path in expected_dam_paths if path is not None]\n",
    "        \n",
    "        # Perform prediction\n",
    "        prediction = classifier(test_image_path)[0]  # Assuming single image batch\n",
    "        predicted_label = prediction['label']\n",
    "        predicted_score = prediction['score']\n",
    "        \n",
    "        # Get predicted DAM image path\n",
    "        print(f\"predicted_label: {predicted_label}\")\n",
    "        predicted_dam_path = label_to_original_path.get(predicted_label, None)\n",
    "        \n",
    "        # Load images\n",
    "        try:\n",
    "            test_image = Image.open(test_image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading test image {test_image_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        expected_images = []\n",
    "        for path in expected_dam_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                expected_images.append(img)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading expected DAM image {path}: {e}\")\n",
    "        \n",
    "        if predicted_dam_path:\n",
    "            try:\n",
    "                predicted_image = Image.open(predicted_dam_path).convert('RGB')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading predicted DAM image {predicted_dam_path}: {e}\")\n",
    "                predicted_image = None\n",
    "        else:\n",
    "            predicted_image = None\n",
    "        \n",
    "        # Display images\n",
    "        num_expected = len(expected_images)\n",
    "        num_cols = 2 + num_expected  # Test image, Predicted, Expected(s)\n",
    "        plt.figure(figsize=(5 * num_cols, 5))\n",
    "        \n",
    "        # Display Test Image\n",
    "        plt.subplot(1, num_cols, 1)\n",
    "        plt.imshow(test_image)\n",
    "        plt.title(\"Test Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Display Predicted Image\n",
    "        if predicted_image:\n",
    "            plt.subplot(1, num_cols, 2)\n",
    "            plt.imshow(predicted_image)\n",
    "            plt.title(f\"Predicted: {predicted_label}\\nScore: {predicted_score:.2f}\")\n",
    "            plt.axis('off')\n",
    "        else:\n",
    "            plt.subplot(1, num_cols, 2)\n",
    "            plt.text(0.5, 0.5, \"Predicted Image Not Found\", horizontalalignment='center', verticalalignment='center')\n",
    "            plt.title(\"Predicted Image\")\n",
    "            plt.axis('off')\n",
    "        \n",
    "        # Display Expected Images\n",
    "        for i, expected_img in enumerate(expected_images):\n",
    "            plt.subplot(1, num_cols, 3 + i)\n",
    "            plt.imshow(expected_img)\n",
    "            plt.title(f\"Expected: {expected_labels[i]}\")\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = \"./vit-finetuned\"\n",
    "model.save_pretrained(model_save_path)\n",
    "processor.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model and processor saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
