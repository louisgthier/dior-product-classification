{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas scikit-learn Pillow numpy tqdm rembg matplotlib onnxruntime transformers torch torchvision torchaudio\n",
    "%pip install kornia timm # RMBG-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModelForImageSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to directories\n",
    "dam_dir = 'data/DAM'\n",
    "test_dir = 'data/test_image_headmind'\n",
    "extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "\n",
    "# Get list of image file paths for DAM and Test\n",
    "dam_images = glob(os.path.join(dam_dir, '*.jpeg'))\n",
    "dam_images.sort()\n",
    "\n",
    "test_images = []\n",
    "for ext in extensions:\n",
    "    pattern = os.path.join(test_dir, ext)\n",
    "    test_images.extend(glob(pattern))\n",
    "test_images.sort()\n",
    "\n",
    "# Create DataFrames\n",
    "dam_df = pd.DataFrame({'image_path': dam_images})\n",
    "test_df = pd.DataFrame({'image_path': test_images})\n",
    "\n",
    "print(\"DAM DataFrame:\")\n",
    "print(dam_df.head())\n",
    "\n",
    "print(\"\\nTest DataFrame:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import preprocess_image\n",
    "import utils.preprocessing\n",
    "import importlib\n",
    "importlib.reload(utils.preprocessing)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps as PIL_ImageOps\n",
    "\n",
    "BASE_CACHE_DIR = '.cache'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "sample_test_path = test_df['image_path'].iloc[60]\n",
    "print(f\"Sample test path: {sample_test_path}\")\n",
    "extracted_object = preprocess_image(sample_test_path, background_removal=\"RMBG_2\")\n",
    "\n",
    "if extracted_object is None:\n",
    "    print(\"No object found in the image at path:\", sample_test_path)\n",
    "\n",
    "# Display the original and extracted object\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(PIL.ImageOps.exif_transpose(Image.open(sample_test_path)))\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(extracted_object)\n",
    "plt.title(\"Extracted Object\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# for i in tqdm(range(len(dam_df))):\n",
    "#     sample_test_path = dam_df['image_path'].iloc[i]\n",
    "#     extracted_object = preprocess_image(sample_test_path)\n",
    "\n",
    "#     if extracted_object is None:\n",
    "#         print(\"No object found in the image at path:\", sample_test_path)\n",
    "#         continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import importlib\n",
    "import json\n",
    "\n",
    "import utils.models\n",
    "import utils.models.nomic_embed_vision_model\n",
    "importlib.reload(utils.models.base_model)\n",
    "importlib.reload(utils.models.dinov2_model)\n",
    "importlib.reload(utils.models.facebook_vitmsn_model)\n",
    "importlib.reload(utils.models.google_vit_model)\n",
    "importlib.reload(utils.models.microsoft_resnet_model)\n",
    "importlib.reload(utils.models.openai_clip_model)\n",
    "importlib.reload(utils.models.fashion_clip_model)\n",
    "importlib.reload(utils.models.nomic_embed_vision_model)\n",
    "importlib.reload(utils.models)\n",
    "import utils.models\n",
    "from utils.models import DinoV2Model, FacebookViTMSNModel, GoogleViTModel, MicrosoftResNetModel, OpenAIClipModel, FashionCLIPModel, NomicEmbedVisionModel\n",
    "\n",
    "\n",
    "BACKGROUND_REMOVAL_METHOD = 'RMBG_2' # 'rembg', 'RMBG_2'\n",
    "AUGMENT_WITH_3D_MODEL = False\n",
    "\n",
    "SELECTED_MODEL = GoogleViTModel\n",
    "EMBEDDING_AGGREGRATION_METHOD = \"mean\" # None, 'mean'\n",
    "COSINE_SIMILARITY = True # False = Euclidean distance\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings\n",
    "\n",
    "# Select the desired model by key\n",
    "selected_model_key = SELECTED_MODEL.__name__\n",
    "model = SELECTED_MODEL()\n",
    "\n",
    "# Define embeddings file path\n",
    "embeddings_file = os.path.join(BASE_CACHE_DIR, \"embeddings\", f'dam_features-{selected_model_key}-{BACKGROUND_REMOVAL_METHOD}.pkl')\n",
    "\n",
    "if \"dam_features_2d\" in locals():\n",
    "    del dam_features_2d\n",
    "if \"dam_features_3d\" in locals():\n",
    "    del dam_features_3d\n",
    "\n",
    "# Aggregate the embedding of an image by averaging the embeddings of its patches\n",
    "def aggregate_embedding(embedding):\n",
    "    if len(embedding.shape) >= 2:\n",
    "        embedding = embedding.squeeze()\n",
    "        if EMBEDDING_AGGREGRATION_METHOD == 'mean':\n",
    "            embedding = np.mean(embedding, axis=0).reshape(1, -1)\n",
    "    else:\n",
    "        embedding = embedding.reshape(1, -1)\n",
    "    return embedding\n",
    "\n",
    "if os.path.exists(embeddings_file):\n",
    "    if \"dam_features_2d\" not in locals():\n",
    "        with open(embeddings_file, 'rb') as f:\n",
    "            dam_features_2d = pickle.load(f)\n",
    "    print(f\"Loaded precomputed DAM features from {embeddings_file}.\")\n",
    "else:\n",
    "    dam_features_2d = {}\n",
    "    batch_size_2d = 32  # Adjust batch size according to your memory and performance requirements\n",
    "    images_batch_2d = []\n",
    "    paths_batch_2d = []\n",
    "\n",
    "    for idx, row in tqdm(dam_df.iterrows(), total=len(dam_df), desc=\"Extracting DAM features (2D)\"):\n",
    "        img_path = row['image_path']\n",
    "        img = preprocess_image(img_path, BACKGROUND_REMOVAL_METHOD)\n",
    "        if img is None:\n",
    "            # Skip if no object found\n",
    "            continue\n",
    "\n",
    "        images_batch_2d.append(img)\n",
    "        paths_batch_2d.append(img_path)\n",
    "\n",
    "        # If batch size is reached, process the batch\n",
    "        if len(images_batch_2d) == batch_size_2d:\n",
    "            # Extract features for the current batch\n",
    "            feats_batch = model.extract_features(images_batch_2d)\n",
    "            # Store features with their corresponding paths\n",
    "            for path, feat in zip(paths_batch_2d, feats_batch):\n",
    "                dam_features_2d[path] = feat\n",
    "\n",
    "            # Clear batches for the next set\n",
    "            images_batch_2d = []\n",
    "            paths_batch_2d = []\n",
    "\n",
    "    # Process any remaining images in the last batch\n",
    "    if images_batch_2d:\n",
    "        feats_batch = model.extract_features(images_batch_2d)\n",
    "        for path, feat in zip(paths_batch_2d, feats_batch):\n",
    "            dam_features_2d[path] = feat\n",
    "\n",
    "    # Save the extracted 2D features to a pickle file\n",
    "    os.makedirs(os.path.dirname(embeddings_file), exist_ok=True)\n",
    "    with open(embeddings_file, 'wb') as f:\n",
    "        pickle.dump(dam_features_2d, f)\n",
    "    print(f\"Extracted features for DAM images (2D) and saved to {embeddings_file}.\")\n",
    "\n",
    "feature_selection_coefficients = np.ones(next(iter(dam_features_2d.values())).shape[-1], dtype=np.float32)\n",
    "print(f\"Feature selection coefficients shape: {feature_selection_coefficients.shape}\")\n",
    "\n",
    "# Augment the data with the 3D model features\n",
    "\n",
    "if AUGMENT_WITH_3D_MODEL:\n",
    "    embeddings_file_3d = os.path.join(BASE_CACHE_DIR, \"embeddings\", f'dam_features-{selected_model_key}-rembg-3d.pkl')\n",
    "    \n",
    "    if os.path.exists(embeddings_file_3d):\n",
    "        if \"dam_features_3d\" not in locals():\n",
    "            with open(embeddings_file_3d, 'rb') as f:\n",
    "                dam_features_3d = pickle.load(f)\n",
    "        print(f\"Loaded precomputed DAM features (3D) from {embeddings_file_3d}.\")\n",
    "    else:\n",
    "        dam_features_3d = {}\n",
    "        batch_size = 32  # Adjust batch size according to your memory and performance requirements\n",
    "        images_batch = []\n",
    "        paths_batch = []\n",
    "        \n",
    "        for idx, row in tqdm(dam_df.iterrows(), total=len(dam_df), desc=\"Extracting DAM features (3D)\"):\n",
    "            img_path = row['image_path']\n",
    "            dam_id = os.path.basename(img_path).split('.')[0]\n",
    "            \n",
    "            for i in range(1, 9):\n",
    "                img_path_3d = f\"{BASE_CACHE_DIR}/TRELLIS/{dam_id}/{dam_id}-{i}.png\"\n",
    "                if not os.path.exists(img_path_3d):\n",
    "                    continue\n",
    "                img = preprocess_image(img_path_3d, \"rembg\")\n",
    "                if img is None:\n",
    "                    # Skip if no object found\n",
    "                    continue\n",
    "                \n",
    "                # Single processing\n",
    "                # feat = model.extract_features(img)\n",
    "                # dam_features_3d[img_path_3d] = feat\n",
    "                \n",
    "                # Batch processing\n",
    "                images_batch.append(img)\n",
    "                paths_batch.append(img_path_3d)\n",
    "\n",
    "                # If batch size is reached, process the batch\n",
    "                if len(images_batch) == batch_size:\n",
    "                    # Extract features for the current batch\n",
    "                    feats_batch = model.extract_features(images_batch)\n",
    "                    # Store features with their corresponding paths\n",
    "                    for path, feat in zip(paths_batch, feats_batch):\n",
    "                        dam_features_3d[path] = feat\n",
    "\n",
    "                    # Clear batches for the next set\n",
    "                    images_batch = []\n",
    "                    paths_batch = []\n",
    "        \n",
    "        # Process any remaining images in the last batch\n",
    "        if images_batch:\n",
    "            feats_batch = model.extract_features(images_batch)\n",
    "            for path, feat in zip(paths_batch, feats_batch):\n",
    "                dam_features_3d[path] = feat\n",
    "        \n",
    "        os.makedirs(os.path.dirname(embeddings_file_3d), exist_ok=True)\n",
    "        with open(embeddings_file_3d, 'wb') as f:\n",
    "            pickle.dump(dam_features_3d, f)\n",
    "        print(f\"Extracted features for DAM images (3D) and saved to {embeddings_file_3d}.\")\n",
    "        \n",
    "    merging_coefficients = np.ones(next(iter(dam_features_3d.values())).shape[-1], dtype=np.float32) * 0.5\n",
    "\n",
    "    # Map 3D features to their corresponding 2D features\n",
    "    map_3d_feature_to_dam_feature = {}\n",
    "    combined_features_list = []\n",
    "    dam_paths = []\n",
    "\n",
    "    for path_3d, feat_3d in dam_features_3d.items():\n",
    "        # Extract the DAM ID from the 3D path\n",
    "        dam_id = os.path.basename(os.path.dirname(path_3d))\n",
    "        # Construct the corresponding original DAM image path\n",
    "        corresponding_path = os.path.join(\"data\", \"DAM\", f\"{dam_id}.jpeg\")\n",
    "        \n",
    "        if corresponding_path in dam_features_2d:\n",
    "            map_3d_feature_to_dam_feature[path_3d] = corresponding_path\n",
    "            feat_2d = dam_features_2d[corresponding_path]\n",
    "            \n",
    "            # Correct Feature Aggregation:\n",
    "            # Multiply 3D features by merging_coefficients and 2D features by (1 - merging_coefficients)\n",
    "            combined_feat = (aggregate_embedding(feat_3d) * merging_coefficients) + (aggregate_embedding(feat_2d) * (1 - merging_coefficients))\n",
    "            \n",
    "            # Apply feature selection coefficients\n",
    "            combined_feat = combined_feat * feature_selection_coefficients  # Ensure broadcasting is handled correctly\n",
    "            \n",
    "            combined_features_list.append(combined_feat)\n",
    "            dam_paths.append(path_3d)\n",
    "        else:\n",
    "            print(f\"No corresponding DAM feature found for 3D image {path_3d}.\")\n",
    "else:\n",
    "    # Initialize lists to hold aggregated features and their corresponding paths\n",
    "    combined_features_list = []\n",
    "    dam_paths = []\n",
    "    \n",
    "    print(list(dam_features_2d.values())[0].shape)\n",
    "    \n",
    "    # Iterate over each 2D feature in dam_features_2d\n",
    "    for path, feat in tqdm(dam_features_2d.items(), desc=\"Building DAM feature matrix (2D)\"):\n",
    "        # Aggregate the embedding (e.g., take the mean if aggregation method is 'mean')\n",
    "        aggregated_feat = aggregate_embedding(feat) * feature_selection_coefficients  # Apply feature selection coefficients\n",
    "        \n",
    "        # Append the aggregated feature and its path to the respective lists\n",
    "        combined_features_list.append(aggregated_feat)\n",
    "        dam_paths.append(path)\n",
    "    \n",
    "# Convert the list of aggregated features into a NumPy array\n",
    "dam_features_matrix = np.vstack(combined_features_list)  # Shape: (num_dam, feature_dim)\n",
    "dam_feature_paths = np.array(dam_paths)\n",
    "\n",
    "print(f\"Built DAM feature matrix from 2D features with shape {dam_features_matrix.shape}.\")\n",
    "\n",
    "# Normalize features if using cosine similarity\n",
    "if COSINE_SIMILARITY:\n",
    "    # Compute the L2 norms of each feature vector\n",
    "    norms = np.linalg.norm(dam_features_matrix, axis=1, keepdims=True)\n",
    "    # Avoid division by zero by setting zero norms to one\n",
    "    norms[norms == 0] = 1\n",
    "    # Normalize the feature matrix\n",
    "    dam_features_matrix = dam_features_matrix / norms\n",
    "    print(\"Normalized DAM feature matrix for cosine similarity.\")\n",
    "\n",
    "# Optionally, convert to float32 for memory and computational efficiency\n",
    "dam_features_matrix = dam_features_matrix.astype(np.float32)\n",
    "\n",
    "# dam_features = dam_features_2d\n",
    "# dam_features = dam_features_2d | dam_features_3d\n",
    "# dam_features = dam_features_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the labels CSV into a DataFrame\n",
    "labels_df = pd.read_csv('labels/handmade_test_labels.csv')\n",
    "\n",
    "# Create a dictionary mapping each test image filename to a list of reference labels\n",
    "labels_dict = {}\n",
    "for _, row in labels_df.iterrows():\n",
    "    image_name = row['image'].strip()\n",
    "    # Split the reference column by '/' to handle multiple labels, remove empty strings and spaces\n",
    "    references = [ref.strip() for ref in str(row['reference']).split('/') if ref.strip() and ref.strip() != '?']\n",
    "    labels_dict[image_name] = references\n",
    "\n",
    "# For debugging, you can print a sample of the dictionary\n",
    "print(dict(list(labels_dict.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "# Initialize accuracy counters\n",
    "total_queries = 0\n",
    "correct_top1 = 0\n",
    "correct_top3 = 0\n",
    "correct_top5 = 0\n",
    "\n",
    "# This dictionary will store the index of the correct match in each query if found\n",
    "correct_match_indices = {}\n",
    "\n",
    "# Set the desired number of top matches to display\n",
    "top_n = 10\n",
    "max_height = 5\n",
    "\n",
    "results = []\n",
    "\n",
    "# Start Processing Test Images\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing test images\"):\n",
    "    test_path = row['image_path']\n",
    "    \n",
    "    total_queries += 1\n",
    "    \n",
    "    # Preprocess Test Image\n",
    "    t = time.time()\n",
    "    test_obj = preprocess_image(test_path, BACKGROUND_REMOVAL_METHOD)  # Adjust if needed\n",
    "    if test_obj is None:\n",
    "        print(f\"Preprocessing failed for {test_path}. Skipping.\")\n",
    "        continue\n",
    "    preprocessing_time = time.time() - t\n",
    "    print(f\"Preprocessing time: {preprocessing_time:.4f} seconds\")\n",
    "    \n",
    "    # Extract Features\n",
    "    t = time.time()\n",
    "    test_feat = model.extract_features(test_obj)  # Adjust if batch processing is needed\n",
    "    feature_extraction_time = time.time() - t\n",
    "    print(f\"Feature extraction time: {feature_extraction_time:.4f} seconds\")\n",
    "    \n",
    "    # Aggregate Features\n",
    "    t = time.time()\n",
    "    test_feat = aggregate_embedding(test_feat)  # Adjust as per your aggregation method\n",
    "    \n",
    "    if AUGMENT_WITH_3D_MODEL:\n",
    "        # If test features have both 3D and 2D, apply coefficients accordingly\n",
    "        # Here, assuming you have separate test_feat_3d and test_feat_2d\n",
    "        # If not, adjust based on your actual feature extraction process\n",
    "        \n",
    "        # Example:\n",
    "        # test_feat_combined = (test_feat_3d * merging_coefficients) + (test_feat_2d * (1 - merging_coefficients))\n",
    "        \n",
    "        # If only 2D features are available for test images:\n",
    "        test_feat_combined = test_feat * feature_selection_coefficients  # Only 2D features\n",
    "    else:\n",
    "        test_feat_combined = test_feat * feature_selection_coefficients\n",
    "    \n",
    "    aggregation_time = time.time() - t\n",
    "    print(f\"Feature aggregation time: {aggregation_time:.4f} seconds\")\n",
    "    \n",
    "    # Normalize Test Feature if using cosine similarity\n",
    "    if COSINE_SIMILARITY:\n",
    "        norm = np.linalg.norm(test_feat_combined)\n",
    "        if norm == 0:\n",
    "            norm = 1\n",
    "        test_feat_combined = test_feat_combined / norm\n",
    "    \n",
    "    # Convert to NumPy array\n",
    "    test_feat_combined = test_feat_combined.astype(np.float32).squeeze()\n",
    "    \n",
    "    # Compute Similarities\n",
    "    t = time.time()\n",
    "    if COSINE_SIMILARITY:\n",
    "        # Compute cosine similarity using dot product since features are normalized\n",
    "        similarities = np.dot(dam_features_matrix, test_feat_combined)\n",
    "    else:\n",
    "        # Compute Euclidean distances\n",
    "        differences = dam_features_matrix - test_feat_combined\n",
    "        similarities = -np.linalg.norm(differences, axis=1)  # Negative for consistency with similarity\n",
    "    similarity_time = time.time() - t\n",
    "    print(f\"Similarity computation time: {similarity_time:.4f} seconds\")\n",
    "    \n",
    "    # Retrieve Top-N Matches\n",
    "    t = time.time()\n",
    "    if top_n < len(similarities):\n",
    "        top_indices = np.argpartition(-similarities, top_n)[:top_n]\n",
    "        top_similarities = similarities[top_indices]\n",
    "        sorted_top_indices = top_indices[np.argsort(-top_similarities)]\n",
    "    else:\n",
    "        sorted_top_indices = np.argsort(-similarities)\n",
    "    sorted_matches = list(zip(dam_feature_paths[sorted_top_indices], similarities[sorted_top_indices]))\n",
    "    sorting_time = time.time() - t\n",
    "    print(f\"Sorting time: {sorting_time:.4f} seconds\")\n",
    "    \n",
    "    # Store Results\n",
    "    results.append({\n",
    "        'test_image': test_path,\n",
    "        'top_matches': sorted_matches[:top_n]\n",
    "    })\n",
    "    \n",
    "    # Retrieve True Reference Labels\n",
    "    test_image_name = os.path.basename(test_path)\n",
    "    true_references = labels_dict.get(test_image_name, [])\n",
    "    \n",
    "    # Check Positions of Correct Matches\n",
    "    found_indices = []\n",
    "    top_n_matches = []\n",
    "    code_duplicates = set()\n",
    "    for i, (match_path, sim_score) in enumerate(sorted_matches):\n",
    "        predicted_code = os.path.splitext(os.path.basename(match_path))[0]\n",
    "        if \"-\" in predicted_code:\n",
    "            predicted_code = predicted_code.split(\"-\")[0]\n",
    "        \n",
    "        if predicted_code in code_duplicates:\n",
    "            continue\n",
    "        \n",
    "        if predicted_code in true_references:\n",
    "            found_indices.append(len(top_n_matches))\n",
    "        \n",
    "        code_duplicates.add(predicted_code)\n",
    "        top_n_matches.append((match_path, sim_score))\n",
    "        if len(top_n_matches) >= top_n:\n",
    "            break\n",
    "\n",
    "    # Update Accuracy Counters\n",
    "    if found_indices:\n",
    "        correct_index = min(found_indices)\n",
    "        correct_match_indices[test_image_name] = correct_index\n",
    "        if correct_index < 1:\n",
    "            correct_top1 += 1\n",
    "        if correct_index < 3:\n",
    "            correct_top3 += 1\n",
    "        if correct_index < 5:\n",
    "            correct_top5 += 1\n",
    "    \n",
    "    # Visualization (Optional: Can be skipped for performance)\n",
    "    # Determine number of columns for grid specification\n",
    "    ncols = min(max(2, top_n), 5)  # Ensure at least 2 columns\n",
    "    nrows = math.ceil(top_n / ncols) + 1  # Add 1 for original and extracted images\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(4 * ncols, max_height * nrows))\n",
    "    gs = fig.add_gridspec(nrows, ncols)\n",
    "    \n",
    "    # Subplot for Original Image\n",
    "    ax_orig = fig.add_subplot(gs[0, 0])\n",
    "    ax_orig.imshow(PIL_ImageOps.exif_transpose(Image.open(test_path)))\n",
    "    ax_orig.set_title(\"Original Image\")\n",
    "    ax_orig.axis('off')\n",
    "    \n",
    "    # Subplot for Extracted Object\n",
    "    ax_ext = fig.add_subplot(gs[0, 1])\n",
    "    ax_ext.imshow(test_obj)\n",
    "    ax_ext.set_title(\"Extracted Object\")\n",
    "    ax_ext.axis('off')\n",
    "    \n",
    "    # Leave remaining cells in the first row empty\n",
    "    for col in range(2, ncols):\n",
    "        ax_empty = fig.add_subplot(gs[0, col])\n",
    "        ax_empty.axis('off')\n",
    "    \n",
    "    # Display Top-N Matches\n",
    "    for i, (match_path, sim_score) in enumerate(top_n_matches[:top_n]):\n",
    "        ax = fig.add_subplot(gs[i // ncols + 1, i % ncols])\n",
    "        img_match = Image.open(match_path)\n",
    "        img_match = PIL_ImageOps.exif_transpose(img_match)\n",
    "        ax.imshow(img_match)\n",
    "        ax.set_title(f\"Match {i+1}\\nSim: {sim_score:.4f}\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Compute and Display Overall Accuracy Metrics\n",
    "    if total_queries > 0:\n",
    "        accuracy_top1 = correct_top1 / total_queries\n",
    "        accuracy_top3 = correct_top3 / total_queries\n",
    "        accuracy_top5 = correct_top5 / total_queries\n",
    "\n",
    "        print(f\"Total queries processed: {total_queries}\")\n",
    "        print(f\"Top-1 Accuracy: {accuracy_top1:.2%} ({correct_top1} correct)\")\n",
    "        print(f\"Top-3 Accuracy: {accuracy_top3:.2%} ({correct_top3} correct)\")\n",
    "        print(f\"Top-5 Accuracy: {accuracy_top5:.2%} ({correct_top5} correct)\")\n",
    "    else:\n",
    "        print(\"No queries were processed.\")\n",
    "    \n",
    "    # Optionally print the indices where correct matches were found for each image\n",
    "    if test_image_name in correct_match_indices:\n",
    "        print(f\"Correct match found at top-{correct_match_indices[test_image_name]+1}\")\n",
    "        \n",
    "    # Display Match Information in Console\n",
    "    tqdm.write(f\"Processed {test_path} -> Top {top_n} matches:\")\n",
    "    for match in top_n_matches[:top_n]:\n",
    "        tqdm.write(f\"\\tMatch: {match[0]}, Similarity: {match[1]:.4f}\")\n",
    "\n",
    "# After all test images are processed, save the benchmark\n",
    "filename = f\"{type(model).__name__}-{BACKGROUND_REMOVAL_METHOD}-{EMBEDDING_AGGREGRATION_METHOD}-{'euclidean' if not COSINE_SIMILARITY else 'cosine'}\"\n",
    "if AUGMENT_WITH_3D_MODEL:\n",
    "    filename += \"-3d\"\n",
    "filename += \".json\"\n",
    "\n",
    "benchmark = {\n",
    "    \"top_1_accuracy\": accuracy_top1 if total_queries > 0 else 0.0,\n",
    "    \"top_3_accuracy\": accuracy_top3 if total_queries > 0 else 0.0,\n",
    "    \"top_5_accuracy\": accuracy_top5 if total_queries > 0 else 0.0\n",
    "}\n",
    "\n",
    "os.makedirs(\"benchmarks\", exist_ok=True)\n",
    "with open(os.path.join(\"benchmarks\", filename), 'w') as f:\n",
    "    json.dump(benchmark, f)\n",
    "\n",
    "print(f\"Benchmark results saved to benchmarks/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Assuming 'device' is predefined, along with labels_dict and other necessary functions/variables (preprocess_image, model, aggregate_embedding, etc.)\n",
    "\n",
    "# Prepare the first 20 images for gradient descent optimization.\n",
    "shuffled_indices = list(range(len(test_df)))\n",
    "random.shuffle(shuffled_indices)\n",
    "train_indices = shuffled_indices[:20]\n",
    "train_samples = test_df.iloc[train_indices]\n",
    "\n",
    "# Initialize coefficients as torch parameters and move them to the specified device.\n",
    "merging_coefficients = torch.nn.Parameter(torch.ones(768, device=device) * 0.5)  \n",
    "feature_selection_coefficients = torch.nn.Parameter(torch.ones(768, device=device) * 0.5)\n",
    "\n",
    "# Define an optimizer for both coefficient vectors.\n",
    "optimizer = optim.Adam([merging_coefficients, feature_selection_coefficients], lr=0.01)\n",
    "\n",
    "# Define a cosine similarity function and move it to the device.\n",
    "cos = nn.CosineSimilarity(dim=1).to(device)\n",
    "\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "\n",
    "all_dam_paths = list(dam_features_3d.keys())\n",
    "\n",
    "# Check if the variable is already defined;\n",
    "if not 'cached_test_features' in locals():\n",
    "    cached_test_features = {}\n",
    "\n",
    "# Cache all test features for faster processing\n",
    "for _, row in test_df.iterrows():\n",
    "    if row['image_path'] in cached_test_features:\n",
    "        continue\n",
    "    test_path = row['image_path']\n",
    "    test_obj = preprocess_image(test_path)\n",
    "    test_feat = model.extract_features(test_obj)\n",
    "    test_feat = aggregate_embedding(test_feat)  # shape (1, 197, 768)\n",
    "    cached_test_features[test_path] = test_feat\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for _, row in train_samples.iterrows():\n",
    "        test_path = row['image_path']\n",
    "        \n",
    "        test_feat = cached_test_features[test_path]\n",
    "        \n",
    "        # Retrieve references for the current test image\n",
    "        test_image_name = os.path.basename(test_path)\n",
    "        references = labels_dict.get(test_image_name, [])\n",
    "        if not references:\n",
    "            continue\n",
    "        \n",
    "        # Choose one correct reference for this iteration\n",
    "        correct_reference = random.choice(references)\n",
    "        correct_dam_3d_path = f\".cache/TRELLIS/{correct_reference}/{correct_reference}-{random.randint(1, 8)}.png\"\n",
    "        \n",
    "        if correct_dam_3d_path not in dam_features_3d:\n",
    "            continue\n",
    "        \n",
    "        # Retrieve and process correct DAM features\n",
    "        dam_feat_3d = dam_features_3d[correct_dam_3d_path]\n",
    "        dam_feat_2d = dam_features_2d[map_3d_feature_to_dam_feature[correct_dam_3d_path]]\n",
    "        dam_feat_3d = aggregate_embedding(dam_feat_3d)\n",
    "        dam_feat_2d = aggregate_embedding(dam_feat_2d)\n",
    "        \n",
    "        # Create tensors and move them to device\n",
    "        dam_feat_3d_tensor = torch.tensor(dam_feat_3d, dtype=torch.float32, device=device)\n",
    "        dam_feat_2d_tensor = torch.tensor(dam_feat_2d, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Expand merging coefficients for broadcasting (already on device)\n",
    "        merge_coeff_expanded = merging_coefficients.view(1, 768)\n",
    "        \n",
    "        # Combine features using merging coefficients\n",
    "        combined_correct = dam_feat_3d_tensor * merge_coeff_expanded + dam_feat_2d_tensor * (1 - merge_coeff_expanded)\n",
    "        \n",
    "        # Convert test feature to torch tensor and move to device\n",
    "        test_feat_tensor = torch.tensor(test_feat, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Expand feature selection coefficients for broadcasting (already on device)\n",
    "        feat_sel_expanded = feature_selection_coefficients.view(1, 768)\n",
    "        \n",
    "        # Apply feature selection to test and combined embeddings\n",
    "        weighted_test = test_feat_tensor * feat_sel_expanded\n",
    "        weighted_correct = combined_correct * feat_sel_expanded\n",
    "        \n",
    "        # Compute similarity with correct DAM using weighted features\n",
    "        similarity_correct = cos(weighted_test, weighted_correct).mean()\n",
    "        \n",
    "        positive_distance = 1 - similarity_correct # Cosine distance between the anchor (weighted_correct) and positive sample (weighted_test)\n",
    "        \n",
    "        # Sample a negative DAM that is not in the reference list\n",
    "        negative_dam_path = None\n",
    "        attempts = 0\n",
    "        while attempts < 10:\n",
    "            candidate = random.choice(list(train_samples['image_path']))\n",
    "            if candidate != test_path:\n",
    "                negative_dam_path = candidate\n",
    "                break\n",
    "            attempts += 1\n",
    "        \n",
    "        # If a valid negative sample is found\n",
    "        if negative_dam_path and negative_dam_path in cached_test_features:\n",
    "            neg_dam_feat = cached_test_features[negative_dam_path]\n",
    "            \n",
    "            neg_dam_feat_tensor = torch.tensor(neg_dam_feat, dtype=torch.float32, device=device)\n",
    "            \n",
    "            weighted_negative = neg_dam_feat_tensor * feat_sel_expanded\n",
    "            \n",
    "            similarity_negative = cos(weighted_correct, weighted_negative).mean()\n",
    "            negative_distance = 1 - similarity_negative # Cosine distance between the anchor (weighted_correct) and negative sample (weighted_negative)\n",
    "        else:\n",
    "            print(\"No valid negative sample found.\")\n",
    "        \n",
    "        loss  = torch.clamp(positive_distance - negative_distance + 0.2, min=0.0)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        merging_coefficients.clamp_(0.0, 1.0)\n",
    "        feature_selection_coefficients.clamp_(0.0, 1.0)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_samples)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, epoch+2), losses, marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "merging_coefficients = merging_coefficients.detach().cpu().numpy()\n",
    "feature_selection_coefficients = feature_selection_coefficients.detach().cpu().numpy()\n",
    "\n",
    "# After training, coefficients are on device; move them to CPU for printing if needed.\n",
    "print(\"Optimized merging coefficients:\", merging_coefficients)\n",
    "print(\"Optimized feature selection coefficients:\", feature_selection_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save coefficients to a file with pickle\n",
    "merging_coefficients_file = os.path.join(BASE_CACHE_DIR, \"coefficients\", f\"merging_coefficients-{selected_model_key}-{BACKGROUND_REMOVAL_METHOD}.pkl\")\n",
    "os.makedirs(os.path.dirname(merging_coefficients_file), exist_ok=True)\n",
    "with open(merging_coefficients_file, 'wb') as f:\n",
    "    pickle.dump(merging_coefficients.cpu().detach().numpy(), f)\n",
    "\n",
    "feature_selection_coefficients_file = os.path.join(BASE_CACHE_DIR, \"coefficients\", f\"feature_selection_coefficients-{selected_model_key}-{BACKGROUND_REMOVAL_METHOD}.pkl\")\n",
    "os.makedirs(os.path.dirname(feature_selection_coefficients_file), exist_ok=True)\n",
    "with open(feature_selection_coefficients_file, 'wb') as f:\n",
    "    pickle.dump(feature_selection_coefficients.cpu().detach().numpy(), f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
